---
title: 模式识别
description: 课程笔记
author: Hatrix
date: 2025-03-04 17:22:00 +0800
categories: [人工智能]
tags: [课程笔记]
math: true
mermaid: true
---

## 概述

课程内容：基本概念和原理，主要算法，应用技能

模式识别属于控制科学与工程，研究如何使计算机具备模式识别的能力，即如何利用规则完成对外部世界完成分类，更关注的是如何解决工程问题，至于是否选用机器学习的算法，取决于具体要解决的问题。步骤有三：设计分类器(选择一个实现分类的算法)，训练分类器，使用分类器。

![image-20250313222845539](../assets/post-pics/image-20250313222845539.png)

考核：平时 25%(在线课章节测验)，项目实践 35%(个人报告 10%，小组论文 10%，项目成果展示及测试 15%)，期末考试 40%(在线课期末考试 20%+乐学或线下开卷 20%)

**注意：提交的东西如果使用了 AI 需要提交说明**

## 第一章 概述

识别的意思是，给某个事物贴上类别标签，也就是分类，类别也可以抽象为某个概念或定义。识别本质是再认知，现有认知再有识别。识别是对概念的再归类。以共同的特征建立类别，识别的基础是相似性。而根据相似性进行识别就有可能出现错误。模式是可以用来给事物代表归类的特征。模式识别的意思是对事物进行概念归类。模式识别的过程仅仅根据特征进行判定，不一定能获知事物本质。

### 人工智能与模式识别

人工智能大概有感知、理解和决策、行动三个方向；主要的人工智能研究集中在第二个方向，以前关注较多的行动，目前研究已经不多了，被整合为具身智能这个概念。模式识别和人工智能有相当大的交集但又互不包含。强化学习和模式识别没有关系(关心下一步的决策，不包含分类)。模式识别也有不包含机器学习的，比如用了模板匹配的光电阅读机。

机器学习的三大任务：分类、回归(拟合)、决策，分类是最核心的任务，另外两个问题在某种程度上都可以划分为分类问题，回归是为了逼近某个数据分布，比如预测就是一个分类问题；强化学习就是一种决策型的任务。模式识别与机器学习在许多算法上是通用的，但模式识别的目的在于使计算机具备识别能力，机器学习的目的在于使计算机更好地理解环境并与环境交互。

### 系统组成

![image-20250313222101151](../assets/post-pics/image-20250313222101151.png)

设计一个模式识别系统，就是要设计分类器的模型、所使用的特征和分类器参数的调整算法。

### 基本概念

将模式信息提取出特征，并将每一个特征作为一个维度构成一个多维特征空间。数值型特征构成向量空间，非数值特征构成集合空间。样本之间的相似程度用特征空间中点的相似程度来计算，每一类样本的聚集区域则表现为特征空间中点的统计分布。数值型特征的相似度度量标准有距离、余弦相似度、皮尔逊相关系数、JACCARD 相似系数等；非数值型距离度量有编辑距离等。

同一类样本点的相似程度用紧致性来衡量，紧致性准则要求样本集中属于同一个类的样本间的相似度，应当远大于属于不同类的样本间的相似度。增加特征空间的维数或进行空间映射变换，是增强该问题模式类的紧致性的常用方法。类别可分型度量是紧致性的量化表示，常用的有基于类内类间距离和基于概率距离两种。当数据维度增加，训练集样本的数量不足而变得稀疏，样本集的紧致性变差，会导致算法性能急剧下降，这被称为维数灾难。

分类器通过某些算法找到自身最优参数的过程，称为分类器的训练或学习。学习分为有监督学习、无监督学习、半监督学习和自监督学习。采用无监督学习的分类器，能够达到更高的智能水平，也是未来模式识别技术发展的主要方向，半监督和自监督是灵活运用有监督和无监督的学习方式。

训练好的分类器对未知新样本正确分类的能力称为泛化能力，而训练好的分类器对未知新样本正确分类的能力大幅度下降的现象称为过拟合，过拟合的来源是误差数据，而误差数据是无法避免的。

## 第二章 特征工程

特征生成和特征降维统称为特征工程。各个维度的特征对于分类的贡献不一，因此可以进行特征降维以删除冗余信息、减少算法计算量和对样本集规模的需求，保留对分类最有效的信息。特征降维的方法主要包括特征选择和特征提取。

特征工程的方法有数据预处理(如数据去重、缺值补充、数据增强、异常值处理等)、特征提取(通过某种变换，将原始特征从高维空间映射到低维空间)、特征选择(从高维特征中挑选出一些最有效的特征，如独立算法、穷举算法、分支界定法、顺序前进/后退、动态顺序前进等)、维度压缩(如 PCA 等)、特征构造、特征变换(如标准化、归一化、对数变换等)、特征编码(如标签编码、独热编码、隶属度函数等)、特征融合。

特征工程的方法需要人工干预，缺少通用的流程和方法。与之对应的端到端技术，就是指跳过特征生成和特征降维环节，直接将具有原始特征的样本数据输入深度神经网络模型，由模型在学习过程中自行提取出对分类最有效的特征，并应用于最终的分类决策。

## 第三章 统计学习理论

机器学习的基本问题可以描述为，根据给定的已知训练样本求取对系统的输出与输入之间依赖关系的估计， 使它能对给定输入下的未知输出做出尽可能准确的预测，即：根据 n 个独立同分布观测样本 $$(x_1,y_1),(x_2,y_2),\cdots,(x_n,y_n)$$，在一组函数$$\{f(x, w)\}$$中寻求最优的一个函数$$\{f(x, w_0)\}$$对依赖关系进行估计，使期望风险最小

$$
R(w)=\int L(y,f(\boldsymbol{x},w))dF(\boldsymbol{x},y) \quad L(y,f(\boldsymbol{x},w))为预测造成的损失，称为损失函数
$$

机器学习不同问题下损失函数的定义如下

- 有监督学习的分类问题，系统输出为类别标签，以二分类为例

  $$
  \begin{aligned}
  L(y,f(\boldsymbol{x},w))=
  \begin{cases}
  0, & y = f(\boldsymbol{x},w)\\
  1, & y \neq f(\boldsymbol{x},w)
  \end{cases}
  \end{aligned}
  $$

  此时的期望风险就是贝叶斯决策中使错误率最小

- 函数拟合中，误差采用最小平方误差

  $$
  L(y,f(\boldsymbol{x},w))=(y - f(\boldsymbol{x},w))^2
  $$

- 对于概率密度估计，目的是确定概率密度函数$$p(x, w)$$
  $$
  L(p(\boldsymbol{x},w))=-\log p(\boldsymbol{x},w)
  $$

### 函数的容量和 VC 维

函数集在一组样本集上可能实现的分类规则数目称为函数集的容量。VC 维的直观定义，对一个指示函数，如果存在$$k$$个样本能够被函数集中的函数按所有可能的$$2^k$$种形式分开，则称函数集能把样本数为$$k$$的样本集打散。指示函数集的 VC 维就是能够打散的最大样本集的样本数目。如果对于任意数目的样本，总能找到一个样本集能够被这个函数集打散，则函数集的 VC 维就是无穷大。函数集的 VC 维并不简单地与函数中自由参数个数有关，而是与函数本身的复杂程度有关。

### 经验风险最小化原则

机器学习就是在函数集中最小化期望风险的期望风险泛函，但这个风险泛函需要对服从联合概率密度$$F(x, y)$$的所有可能样本及其输出值求期望，这在$$F(x, y)$$未知的情况下无法进行。根据大数定律，用算数平均值代替期望，定义经验风险为在训练样本上损失函数的平均，传统的机器学习方法都用这中方式来代替最小化期望风险的目标，这种策略称为经验风险最小化原则(ERM 原则)。

当训练样本数趋向于无穷大时，以经验风险最小化原则进行的学习与期望风险最小的目标是否一致，这称为学习过程的一致性。学习理论关键定理指出，对于有界的损失函数，经验风险最小化学习一致的充分必要条件是，经验风险在如下式意义上一致地收敛于真实风险

$$
\lim_{n\rightarrow\infty}P\left\{\sup_{w_n}\left(R(w_n)-R_{EMP}(w_n)\right)>\varepsilon\right\}=0, \quad \forall\varepsilon > 0
$$

### 结构风险最小化原则

学习机器的实际风险由经验风险(训练误差)和置信范围组成，置信范围和学习机器的 VC 维及训练样本量有关。样本数量较少时置信范围较大，用经验风险最小化取得的最优解可能会有较大的期望风险，即可能推广性较差，因此需要同时最小化经验风险和置信范围，而不是单独最小化经验风险。通过按照 VC 维的大小排序并划分函数子集结构，学习目标化为在函数集中同时进行子集的选择和子集中最优函数的选择。选择最小经验风险与置信范围之和最小的子集，就可以达到期望风险的最小，这个子集中使经验风险最小的函数就是要求的最优函数。这种策略称作结构风险最小化原则(SRM 原则)。

对于样本数给定的问题，学习机器的 VC 维越高(复杂性越高)，置信范围就越大，真实风险与经验风险之间可能的差别越大，推广能力可能越差。因此对于有限样本的任务，应该尽可能选用相对简单的分类器来期望获得尽可能好的推广能力。

### 正则化方法

正则化是解决不适定问题的一类方法(不适定问题是指，即使方程存在唯一解，方程右边的微小扰动$$\vert F - F_{\delta} \vert < \delta$$会带来很大变化；后来发现应该最小化以下的正则化泛函：

$$
R^*(f)=\|Af - F_{\delta}\|^2+\lambda(\delta)\Omega(f)
$$

对于机器学习问题大部分都是用样本特征和对应的观测数据来拟合它们之间的函数关系，所以都属于不适定问题。为解决这个问题，在机器学习模型学习过程中 就必须要引入正则化方法，通常是在模型的损失函数中添加一个正则项(惩罚项)来实现。常见的正则化方法有：Lasso 正则化(L1 正则化)、Tikhonov 正则化(L2 正则化)、弹性网正则化(混合正则化)；对于正则化方式使用的范数，也是一种超参数。

## 第四章 线性分类器

模式识别的决策论方法是指，以一定的规则集将特征空间划分为不同的决策域，决策域包含了样本的分布区域但不等同，判定样本所属的决策域就是分类。判别函数是参数集的函数，是决策边界的方程表示，这个参数集就是权向量，找判别函数就是极小化准则函数的泛函问题。满足判别函数的某个条件则说明属于这一类，否则说明不属于；显然，在二分类问题中，不属于该类则属于另一类。

### 前置知识

#### 线性判别函数

$$
\begin{aligned}
G(\boldsymbol{x}) &= \boldsymbol{w}^T\boldsymbol{x} + w_0\\
\boldsymbol{x}&=[x_1, x_2, \dots, x_n]^T \\
\boldsymbol{w}&=[w_1, w_2, \dots, w_n]^T\\
\end{aligned}
$$

也可写成增广形式，将$$w_0$$加入权向量中，本质相同，处理上简单一些。线性判别函数为一个超平面，权向量$$\boldsymbol{w}$$与该超平面正交，$$\frac{\vert w_0 \vert}{\vert\vert \boldsymbol{w} \vert\vert}$$为超平面到原点距离，判别函数是样本到超平面距离的一种度量。

#### 多分类问题

- 绝对可分：有许多分类不确定的区域，$$m$$个类有$$m$$个判别函数；应用最广泛
- 两两可分：分类不确定区域减少，$$m$$个类有$$\frac{m(m-1)}{2}$$个判别函数
- 最大值可分：样本在每个类的判别函数都有值，归到取值最大的判别函数对应的类中，是两两可分的特殊情况

#### 广义线性化

两个类别线性可分的条件为：分布范围本身是互不重叠的全连通单一凸集。某些线性不可分的问题可通过映射到另一个高维空间转化为线性可分问题，这称为广义线性化。

#### 线性分类器原理

设计：设决策边界方程如下(以增广形式为例)，若问题线性可分则找最优解即可，若非线性可分则问题无解

$$
\begin{aligned}
G_j(\boldsymbol{x})&=\boldsymbol{W}^T\boldsymbol{X}
\end{aligned}
$$

训练：设定一个标量函数$$J(\boldsymbol{w})$$作为准则函数，求解使得准则函数取极小值的权向量$$\boldsymbol{w}$$就是最优解。一般来说无法通过$$\frac{\partial J(\boldsymbol{w})}{\partial \boldsymbol{w}}=0$$直接获得解析解，而是通过逐步逼近的算法根据训练集求取最优的权向量。最常见的逐步逼近算法为梯度下降法，即从初始权向量$$\boldsymbol{w}(0)$$出发，每次沿当前$$J(\boldsymbol{w})$$的负梯度方向前进一步来修正权向量

$$
\begin{aligned}
\boldsymbol{w}(k + 1) &=\boldsymbol{w}(k)-\rho(k + 1)\nabla J(\boldsymbol{w}(k))
\end{aligned}
$$

其中$$\rho(k+1)$$为第$$K+1$$步的步长，又称学习率，是一种超参数，可以给固定值，也可以通过绝对修正、部分修正、最优步长等方式随学习过程变化，较为常用的策略是令$$\rho(k+1) = \frac{\lambda}{k}$$

### 感知器算法

![image-20250322174737994](../assets/post-pics/image-20250322174737994.png)

PA(Perception Algorithm)没有反馈和内部状态，是对多个输入量加权求和再和某个阈值比较决定输出的算法，若干感知器并列，构成单层感知器。对于线性可分的样本，PA 通过调整总可以使权向量收敛到在训练集上不出现分类错误的最优解。对于二分类问题，可以将其中一个类别的增广特征向量取负号，以使用统一的形式表示判别函数对所有训练样本集中样本的取值条件，这称为样本特征向量的规范化。

PA 使用梯度下降法来训练分类器，准则函数如下，其中$$X_0$$为样本集中分类错误的子集，存在错分样本时准则函数大于零，不存在错分样本时则为零

$$
\begin{aligned}
J(\boldsymbol{w})&=\sum_{\boldsymbol{x}\in X_0} (-\boldsymbol{w}^T\boldsymbol{x})\\
\boldsymbol{w}(k + 1) &=\boldsymbol{w}(k)-\rho(k + 1)\nabla J(\boldsymbol{w}(k))\\
&=\boldsymbol{w}(k)+\rho(k + 1)\sum_{\boldsymbol{x}\in X_0} \boldsymbol{x}
\end{aligned}
$$

其中，$$\nabla J(\boldsymbol{w}(k))&=\left(\frac{\partial J(\boldsymbol{w}(k))}{\partial w_1}, \frac{\partial J(\boldsymbol{w}(k))}{\partial w_1}, \dots, \frac{\partial J(\boldsymbol{w}(k))}{\partial w_n}, \frac{\partial J(\boldsymbol{w}(k))}{\partial w_0}\right)=\sum_{\boldsymbol{x}\in X_0} (-\boldsymbol{x})\\$$

梯度下降时，既可以选择批量梯度下降(即每一步同时修正所有分错的样本)，也可以选择随机梯度下降(严格来讲叫顺序梯度下降，即将每次按顺序抽取一个样本，检查是否分错)

PA 的准则函数只与是否能够正确分类有关，不关心能否取得更优的解。

---

单层感知器只能解决线性可分的分类模式问题，要增强分类能力可增加一层或多层隐藏层以构成多层感知器 MLP，这样的结构称为多层前向神经网络，可以解决非线性可分问题，对于任意复杂的分类区域，总可以用多个神经元组成一定的层次结构来实现分类。每一层中的每个单元就是一个人工神经元，其作用相当于特征提取器。

$$
\begin{aligned}
y&=\theta\left\{\sum_{j = 1}^{q}w_j\theta\left(\sum_{h = 1}^{n}v_{jh}x_h + v_{j0}\right)+w_0\right\}
\end{aligned}
$$

![image-20250322175936333](../assets/post-pics/image-20250322175936333.png)

通过基于链式法则的反向传播算法，可以高效计算出误差关于每个权重的梯度，在训练过程中实现对所有参数的更新。另外，各种优化算法(如随机梯度下降及其变种 Adagrad、Adadelta、Adam 等)的提出，也提高了多层感知器参数学习的效率和稳定性。

### LMSE 算法

#### 线性分类器的松弛求解

对于线性不可分的样本集，不等式组不可能同时满足，于是目标转为求解使得错分样本数尽可能少的权向量，具体方法是给定裕度向量$$\boldsymbol{b}$$，对不等式方程组进行松弛化，将求解不等式方程组的问题转化为求解线性方程组的问题

$$
\boldsymbol{X}\boldsymbol{w} = \boldsymbol{b}
$$

定义准则函数为均方误差 MSE

$$
\begin{aligned}
J(\boldsymbol{w}(k))&=\|\boldsymbol{X}\boldsymbol{w}(k)-\boldsymbol{b}\|^2\\
&=\sum_{i = 1}^{l}(\boldsymbol{x}^{(i)T}\boldsymbol{w}(k)-b_i)^2\\
&=\sum_{i = 1}^{l}(\boldsymbol{w}^{T}(k)\boldsymbol{x}^{(i)}-b_i)^2\\
\nabla J(\boldsymbol{w}(k))&=\nabla\sum_{i = 1}^{l}(\boldsymbol{w}^{T}(k)\boldsymbol{x}^{(i)}-b_i)^2\\
&=\sum_{i = 1}^{l}2(\boldsymbol{w}^{T}(k)\boldsymbol{x}^{(i)}-b_i)\boldsymbol{x}^{(i)}\\
&=2\boldsymbol{X}^T(\boldsymbol{X}\boldsymbol{w}(k)-\boldsymbol{b})\\
\boldsymbol{w}(k + 1) &=\boldsymbol{w}(k)-\rho(k + 1)\cdot2\boldsymbol{X}^T(\boldsymbol{X}\boldsymbol{w}(k)-\boldsymbol{b})\\
&=\boldsymbol{w}(k)-\rho'(k + 1)\boldsymbol{X}^T(\boldsymbol{X}\boldsymbol{w}(k)-\boldsymbol{b})
\end{aligned}
$$

该方法除了对线性可分的问题可以收敛外，对于线性不可分问题也可以通过定义误差灵敏度进行近似求解。另外，如果任意给定$$\boldsymbol{b}$$，LMSE 的方法得出的最优权向量可能会将原本线性可分的问题变成线性不可分的问题，即均方误差准则函数取得最小值和线性分类器的不等式方程组成立这两个条件可能无法同时满足。

#### H-K 算法

同时优化权向量$$\boldsymbol{w}$$和裕度向量$$\boldsymbol{b}$$，若两类线性可分，则 LMSE 的准则函数可取得最小值 0，此时决策边界与所有以各样本为中心，以最优裕量向量$$\boldsymbol{b}$$的各分量为半径指标的圆相切。求$$\boldsymbol{b}$$的过程同样可采用梯度法：

$$
\begin{aligned}
\boldsymbol{b}(k + 1) &=\boldsymbol{b}(k)-\eta(k + 1)\left.\nabla J(\boldsymbol{w}(k))\right|_{\boldsymbol{b}}\\
\left.\nabla J(\boldsymbol{w}(k))\right|_{\boldsymbol{b}}&=\frac{\partial\|\boldsymbol{X}\boldsymbol{w}(k)-\boldsymbol{b}\|^2}{\partial\boldsymbol{b}}=-2(\boldsymbol{X}\boldsymbol{w}(k)-\boldsymbol{b})\\
\boldsymbol{b}(k + 1) &=\boldsymbol{b}(k)+\eta(k + 1)\cdot2(\boldsymbol{X}\boldsymbol{w}(k)-\boldsymbol{b}(k))\\
&=\boldsymbol{b}(k)+2\eta(k + 1)(\boldsymbol{X}\boldsymbol{w}(k)-\boldsymbol{b}(k))\\
&=\boldsymbol{b}(k)+2\eta(k + 1)\boldsymbol{e}(k)
\end{aligned}
$$

由于需要保证裕度向量$$\boldsymbol{b}(k + 1)$$始终为正，而误差向量$$\boldsymbol{e}(k)$$正负都有，所以需要修正，一种做法是在$$\boldsymbol{e}(k) > 0$$时进行修正($$\boldsymbol{e}(k) < 0$$说明样本集线性不可分)：

$$
\begin{aligned}
\boldsymbol{b}(k + 1) &= \boldsymbol{b}(k)+\eta(k + 1)(\boldsymbol{e}(k)+|\boldsymbol{e}(k)|)
\end{aligned}
$$

若已知每一步的$$\boldsymbol{b}(k)$$后利用准则函数梯度为 0 来求最优的$$\boldsymbol{w}(k)$$，就可得到一种同时求取最有权向量和最优裕度向量的递推算法，称为 H-K 算法

$$
\begin{aligned}
\left.\nabla J(\boldsymbol{w}(k))\right|_{\boldsymbol{w}(k)}&=2\boldsymbol{X}^T(\boldsymbol{X}\boldsymbol{w}(k)-\boldsymbol{b}) = 0\\
\boldsymbol{w}(k)&=(\boldsymbol{X}^T\boldsymbol{X})^{-1}\boldsymbol{X}^T\boldsymbol{b}(k)\\
\boldsymbol{w}(k + 1)&=(\boldsymbol{X}^T\boldsymbol{X})^{-1}\boldsymbol{X}^T\boldsymbol{b}(k + 1)\\
&=(\boldsymbol{X}^T\boldsymbol{X})^{-1}\boldsymbol{X}^T[\boldsymbol{b}(k)+\eta(k + 1)(\boldsymbol{e}(k)+|\boldsymbol{e}(k)|)]\\
&=(\boldsymbol{X}^T\boldsymbol{X})^{-1}\boldsymbol{X}^T\boldsymbol{b}(k)+\eta(k + 1)(\boldsymbol{X}^T\boldsymbol{X})^{-1}\boldsymbol{X}^T(\boldsymbol{e}(k)+|\boldsymbol{e}(k)|)\\
&=\boldsymbol{w}(k)+\eta(k + 1)(\boldsymbol{X}^T\boldsymbol{X})^{-1}\boldsymbol{X}^T(\boldsymbol{e}(k)+|\boldsymbol{e}(k)|)\\
&=\boldsymbol{w}(k)+\eta(k + 1)(\boldsymbol{X}^T\boldsymbol{X})^{-1}\boldsymbol{X}^T(\boldsymbol{X}\boldsymbol{w}(k)-\boldsymbol{b}(k))+\eta(k + 1)(\boldsymbol{X}^T\boldsymbol{X})^{-1}\boldsymbol{X}^T|\boldsymbol{e}(k)|\\
&=\boldsymbol{w}(k)+\eta(k + 1)[(\boldsymbol{X}^T\boldsymbol{X})^{-1}\boldsymbol{X}^T\boldsymbol{X}\boldsymbol{w}(k)-(\boldsymbol{X}^T\boldsymbol{X})^{-1}\boldsymbol{X}^T\boldsymbol{b}(k)]+\eta(k + 1)(\boldsymbol{X}^T\boldsymbol{X})^{-1}\boldsymbol{X}^T|\boldsymbol{e}(k)|\\
&=\boldsymbol{w}(k)+\eta(k + 1)(\boldsymbol{X}^T\boldsymbol{X})^{-1}\boldsymbol{X}^T|\boldsymbol{e}(k)|
\end{aligned}
$$

### Fisher 算法

将模式样本从高维空间投影到低维空间(比如先尝试投影到一维空间，若效果不佳再投影到二维空间等，每个投影对应构建一个判别函数)，在低维空间实现区分是该算法的主要思想，这个投影方向的选择十分重要。Fisher 算法定义准则函数如下，其中$$m_i$$为各类在$$d$$维特征空间中的样本均值向量，$$M_i$$为映射到一维特征后各类的样本平均值，$$T_i$$为映射后各类样本的类内离散度，$$\boldsymbol{S}_b$$为原 d 维空间的类内样本离散度矩阵，$$\boldsymbol{S}_w$$为原 d 维空间的类间样本总离散度矩阵，使得准则函数取得最大值的权向量记为$$\boldsymbol{W}^*$$

$$
\begin{aligned}
J(\boldsymbol{W})&=\frac{|M_1 - M_2|^2}{T_1^2 + T_2^2}=\frac{\boldsymbol{W}^T\boldsymbol{S}_b\boldsymbol{W}}{\boldsymbol{W}^T\boldsymbol{S}_w\boldsymbol{W}}
\\
m_i&=\frac{1}{N_i}\sum_{X\in w_i}X, \quad i = 1, 2\\
M_i&=\frac{1}{N_i}\sum_{y\in\Omega_i}y, \quad i = 1, 2\\
T_i^2&=\sum_{y\in\Omega_i}(y - M_i)^2, \quad i = 1, 2
\end{aligned}
$$

求解这个$$\boldsymbol{W}^*$$是一个广义瑞利商求极值问题，可通过拉格朗日乘子法求解出

$$
\begin{aligned}
W^*&=S_w^{-1}(m_1 - m_2)
\end{aligned}
$$

Fisher 算法可直接求解权向量，且可推广到多分类问题中；但对于线性不可分的情况，Fisher 算法无法确定分类。确定最优权向量后，Fisher 判别法的判别规则如下，其中$$y_t$$的选择可取映射值的中值、加权平均值、插值、结合先验概率的中值等

$$
\begin{aligned}
Y&=W^TX > y_t \Rightarrow X\in w_1 \\
Y&=W^TX < y_t \Rightarrow X\in w_2
\end{aligned}
$$

### 支持向量机算法

#### 线性 SVM

使得两类训练样本中离超平面最近的样本与超平面的距离达到最大的超平面称为最优分类超平面，也称最大间隔超平面，最优超平面定义的分类决策函数如下

$$
\begin{aligned}
f(\boldsymbol{x})&=\mathrm{sgn}(G_{ij}(\boldsymbol{x}_s))=\mathrm{sgn}((\boldsymbol{W}^T\boldsymbol{x}_s) + w_0)
\end{aligned}
$$

使得分类间隔最大的最优权向量是由距离决策边界最近的样本所决定的，这些少数的样本称为支持向量。分类间隔由支持向量到决策边界的距离决定；当$$|G_{ij}(\boldsymbol{x}_s)|$$为常数时，最大的分类间隔等效于长度最小的权向量

$$
\begin{aligned}
d&=2\frac{|G_{ij}(\boldsymbol{x}_s)|}{\|\boldsymbol{w}\|} \\
\max d&=\max\frac{2|G_{ij}(\boldsymbol{x})|}{\|\boldsymbol{w}\|} \Leftrightarrow \min\|\boldsymbol{w}\|
\end{aligned}
$$

支持向量机在解决有限样本(少量支持向量)、非线性(可以通过广义线性化向高位空间映射)和高维度(核函数可直接计算高维内积)模式识别问题上具有优秀的性能和泛化能力(由线性结构组成，结构风险小)，且能处理本质线性但由于扰动偏差而不完全具有线性可分形式的问题。

对于线性分类问题，支持向量机的求解等价于求解二次规划问题

$$
\begin{aligned}
&\min_{\boldsymbol{w}, w_0} \frac{1}{2}\|\boldsymbol{w}\|^2 \\
&\text{s.t. } y_i[(\boldsymbol{w}\cdot\boldsymbol{x}^{(i)}) + w_0] - 1\geq0, i = 1, 2, \ldots, l
\end{aligned}
$$

这是一个在不等式约束下的优化问题，可通过拉格朗日法求解，最终得最优权向量和分类决策边界如下，其中$$l_s$$为支持向量的个数，$$\boldsymbol{x}_s^{(i)}$$为支持向量；求解得到$$\alpha^{(i)}$$非零项即为支持向量

$$
\begin{aligned}
\boldsymbol{w}&=\sum_{i = 1}^{l}\mathrm{sgn}(G_{ij}(\boldsymbol{x}^{(i)}) - 1)\alpha^{(i)}\boldsymbol{x}_s^{(i)}\\
G_{ij}(\boldsymbol{x})&=\boldsymbol{w}^T\boldsymbol{x}+w_0 = \left[\sum_{i = 1}^{l}\mathrm{sgn}(G_{ij}(\boldsymbol{x}_s^{(i)}) - 1)\alpha^{(i)}\boldsymbol{x}_s^{(i)}\right]^T\boldsymbol{x}+w_0\\
&=\sum_{i = 1}^{l}\mathrm{sgn}(G_{ij}(\boldsymbol{x}_s^{(i)}) - 1)\alpha^{(i)}\boldsymbol{x}_s^{(i)T}\boldsymbol{x}+w_0
\end{aligned}
$$

#### 软间隔 SVM

用于解决由于样本集中的异常点带来的线性不可分问题，通过在约束条件中减去一项正数$$\xi$$，使判别函数的绝对值允许小于 1

$$
\begin{aligned}
\min &\frac{1}{2}\|w\|^{2}+C\sum_{i = 1}^{l}\xi^{(i)}\\
\text{s.t. } &\text{sgn}(G_{y^{(i)}}(x^{(i)}))(w^{T}x^{(i)} + w_{0})\geq1 - \xi^{(i)}
\end{aligned}
$$

在训练之前不知道哪些是支持向量，因此松弛变量$$\xi^{(i)}$$的值并非预先给定，也作为优化的目标，希望尽可能小，在原来的最短权向量的二次优化目标基础上，再加上一项$$C\sum_{i = 1}^{l}\xi^{(i)}$$，$$C$$称为惩罚因子，表示对异常点的容忍程度，$$C$$越大表示对异常点的容忍度越低。

采用松弛变量和惩罚因子的 SVM 仍然是线性分类器，致使付出了经验风险不为 0 的代价来减小采样误差和噪声干扰对分类器训练的影响，以得到泛化能力更好的分类器。

#### 非线性 SVM

对于问题本质上的不可分，使用广义线性化的方式，将特征向量映射到高维特征空间变成线性问题，映射之后，除了类别标签之外，没有用到映射之后的像，用到的只是高维空间中的内积，而核函数可以不经过映射过程，直接计算高维空间中的内积。

当有一个形为$$K(x^{(i)},x^{(j)})$$的标量函数，对样本集中所有样本间的函数值构成的矩阵是半正定的，则该函数为核函数。常用的核函数有

![image-20250322222519408](../assets/post-pics/image-20250322222519408.png)

核函数的选择目前只能靠经验来试，且核函数只是提供一种解决非线性分类问题的可能性。

## 第五章 贝叶斯分类器

贝叶斯分类器是基于统计决策方法实现的概率分类方法。

### 贝叶斯推理原理

推理有确定性推理和概率推理，贝叶斯理论给出了逆概率推理的方法，即已知结果，某个原因有多大的概率发生。

贝叶斯公式如下，其中$$P(B_{i}|A)$$为后验概率，$$P(B_{i})$$为先验概率，$$P(A|B_{j})$$为类条件概率，$$P(A)$$全概率

$$
P(B_{i}|A)=\frac{P(A|B_{i})P(B_{i})}{\sum_{j = 1}^{c}P(A|B_{j})P(B_{j})}=\frac{P(A|B_{i})P(B_{i})}{P(A)}
$$

### 贝叶斯分类

将样本属于某个类作为原因，样本的特征向量取值作为结果，则模式识别的分类决策过程可以看作逆图例过程。这种决策分为确定性分类决策和随机性分类决策，而随机性分类决策就可以利用贝叶斯公式来计算样本属于各类的后验概率，计算公式如下，其中$$P(\omega_{i}|x)$$为后验概率，$$P(\omega_{i})$$为先验概率

$$
P(\omega_{i}|x)=\frac{P(x|\omega_{i})P(\omega_{i})}{\sum_{j = 1}^{c}P(x|\omega_{j})P(\omega_{j})}=\frac{P(x|\omega_{i})P(\omega_{i})}{P(x)}
$$

贝叶斯公式具有以下特点：

- 需要已知先验概率
- 需要按照获得的新信息来修正先验概率
- 这种概率推理存在错误率

### 贝叶斯决策算法

#### 最小错误率贝叶斯分类

等价于最大后验概率分类，分类决策规则为

$$
\begin{aligned}
&\text{两类问题中，当 }P(\omega_{i}|x)>P(\omega_{j}|x)\text{ 时，判决 }x\in\omega_{i}\text{；}\\
&\text{对于多类情况，则当 }P(\omega_{i}|x)=\max_{1\leq j \leq c}P(\omega_{j}|x)\text{ 时，判决 }x\in\omega_{i}\text{。}
\end{aligned}
$$

分类决边界为

$$
P(\omega_{i}|x)=P(\omega_{j}|x)
$$

这样确定的分类决策边界不一定是线性或者是连续的

#### 最小风险贝叶斯分类

将决策风险纳入考虑，有如下定义

- 决策$$\alpha_i$$：把待识别样本归导$$\omega_i$$类中

- 损失$$\lambda_{ij}$$：把真实属于$$\omega_j$$类的样本归类到$$\omega_i$$中带来的损失

- 条件风险$$R(\alpha_i \vert x)$$：对$$x$$采取决策$$\alpha_i$$后可能的总风险，用加权平均计算，权值为样本属于各类的概率，总风险计算公式为
  $$
  R(\alpha_{i}|x)=E[\lambda_{ij}]=\sum_{j = 1}^{c}\lambda_{ij}P(\omega_{j}|x),i = 1,2,\cdots,c
  $$

最小风险贝叶斯分类器的分类决策规则为

$$
\text{若}R(\alpha_{k}|x)=\min_{i = 1,2,\cdots,c}R(\alpha_{i}|x),\text{则}x\in\omega_{k}
$$

当损失取阶跃函数时就是最小错误率贝叶斯分类器，因此最小错误率贝叶斯分类器是最小风险贝叶斯分类器的特例。

#### 朴素贝叶斯分类

不难发现基于贝叶斯公式来估计后验概率的主要困难在于：类条件概率是所有类别上的联合概率，难以从有限的训练样本直接估计而得。因此朴素贝叶斯分类器(Naive Bayes Classifier, NBC)采用了“特征条件独立性假设”对已知类别，假设所有特征在类别已知的条件下是相互独立的，即每个特征独立地对分类结果发生影响。样本类条件概率等于各特征条件概率连乘

$$
p(X|\omega_{i}) = p(x_{1},\cdots,x_{d}|\omega_{i})=\prod_{j = 1}^{d}p(x_{j}|\omega_{i})
$$

在构建分类器时，只需要逐个估计出每个类别的训练样本在每一维特征上的分布，就可以得到每个类别的条件概率密度，大大减少了需要估计参数的数量。朴素贝叶斯分类器可以根据具体问题来确定样本在每一维特征上的分布形式，最常用的一种是假设每一个类别的样本都服从各维特征之间相互独立的高斯分布，其中$$\mu_{ij}$$和$$\sigma_{ij}^2$$为第 i 类样本在第 j 维特征上的均值和方差

$$
p(X|\omega_{i})=\prod_{j = 1}^{d}p(x_{j}|\omega_{i})=\prod_{j = 1}^{d}\left\{\frac{1}{\sqrt{2\pi\sigma_{ij}^{2}}}\exp\left[-\frac{(x_{j}-\mu_{ij})^{2}}{2\sigma_{ij}^{2}}\right]\right\}
$$

$$
\begin{aligned}
g_{i}(X)&=\ln p(X|\omega_{i})+\ln p(\omega_{i})\\
&=\sum_{j = 1}^{d}\left[-\frac{1}{2}\ln 2\pi-\ln\sigma_{ij}-\frac{(x_{j}-\mu_{ij})^{2}}{2\sigma_{ij}^{2}}\right]+\ln p(\omega_{i})\\
&=-\frac{d}{2}\ln 2\pi-\sum_{j = 1}^{d}\ln\sigma_{ij}-\sum_{j = 1}^{d}\frac{(x_{j}-\mu_{ij})^{2}}{2\sigma_{ij}^{2}}+\ln p(\omega_{i})\\
&=\ln p(\omega_{i})-\sum_{j = 1}^{d}\ln\sigma_{ij}-\sum_{j = 1}^{d}\frac{(x_{j}-\mu_{ij})^{2}}{2\sigma_{ij}^{2}} \quad (省略常数项)
\end{aligned}
$$

$$


$$

## 临时记录

Linear Algebra Done Right 及其练习

泛函问题

$$X^\dagger$$

权向量归一化后，超平面的位置由 w_0 决定

线性不可分的条件，凸包，连通性

广义线性化：如果能找到非线性边界，想办法映射到高维，扩维使其线性。支持向量机在理论上就是用的广义线性化

线性分类器本质上只能进行二分类，多分类本质上是多个二分类的组合。绝对可分，多分类组合的不可识别区域非常大。两两可分的不可识别区域相对较小。最大值可分，要看哪一类的值最大，是一种特殊的两两可分。目前支持向量机处理多分类问题仍然采用绝对可分。

判别函数的值代表了距离分类决策边界的距离

训练分类器的一般思路：设定准则函数、求极小值，则求得最优值；改变权向量来优化

变换：增广、规范化

具体求解的算法：感知器算法，最大特点是把生物神经元机理做了数学抽象，把树突收集到的信号与激活阈值进行比较，大于则产生输出。准则函数设为错分样本的集合。用梯度下降法求取，有批量梯度下降法和随机梯度下降法(课程中其实应该叫单词梯度下降法，要把每个样本都用来下降一次)

学习速率设定：固定式、绝对修正、部分修正、变速学习(用的多)、优化学习(把步长也最优化)。下降受样本集影响，不一定都能收敛

线性代数研究的是向量空间及其变换，压缩

关联最密切的是协方差矩阵，特征值分解

泛函：最优化控制里面必讲泛函

核心是首先由函数空间，无穷维的空间，每个向量是一个函数，复返空间

值域在普通的向量空间，泛函就是无穷维空间到这个向量空间的映射，函数空间

每一个函数对应一个实数，研究函数取什么形式，可以使得函数取某一个值，类似于最优化问题

研究函数取什么形式，函数能取得这个值，模型求解就是求泛函的问题。求取什么参数时，准则函数可以求得极值，通过算子求得极值

求求解这个泛函的方法

手写数字识别的流程

![image-20250318163411713](../assets/post-pics/image-20250318163411713.png)
