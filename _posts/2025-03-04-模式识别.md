---
title: 模式识别
description: 课程笔记
author: Hatrix
date: 2025-03-04 17:22:00 +0800
categories: [人工智能]
tags: [课程笔记]
math: true
mermaid: true
---

## 概述

课程内容：基本概念和原理，主要算法，应用技能

模式识别属于控制科学与工程，研究如何使计算机具备模式识别的能力，即如何利用规则完成对外部世界完成分类，更关注的是如何解决工程问题，至于是否选用机器学习的算法，取决于具体要解决的问题。步骤有三：设计分类器（选择一个实现分类的算法），训练分类器，使用分类器。

![image-20250313222845539](../assets/post-pics/image-20250313222845539.png)

考核：平时 25%（在线课章节测验），项目实践 35%（个人报告 10%，小组论文 10%，项目成果展示及测试 15%），期末考试 40%（在线课期末考试 20%+乐学或线下开卷 20%）

**注意：提交的东西如果使用了 AI 需要提交说明**

## 第一章 概述

识别的意思是，给某个事物贴上类别标签，也就是分类，类别也可以抽象为某个概念或定义。识别本质是再认知，现有认知再有识别。识别是对概念的再归类。以共同的特征建立类别，识别的基础是相似性。而根据相似性进行识别就有可能出现错误。模式是可以用来给事物代表归类的特征。模式识别的意思是对事物进行概念归类。模式识别的过程仅仅根据特征进行判定，不一定能获知事物本质。

### 人工智能与模式识别

人工智能大概有感知、理解和决策、行动三个方向；主要的人工智能研究集中在第二个方向，以前关注较多的行动，目前研究已经不多了，被整合为具身智能这个概念。模式识别和人工智能有相当大的交集但又互不包含。强化学习和模式识别没有关系（关心下一步的决策，不包含分类）。模式识别也有不包含机器学习的，比如用了模板匹配的光电阅读机。

机器学习的三大任务：分类、回归（拟合）、决策，分类是最核心的任务，另外两个问题在某种程度上都可以划分为分类问题，回归是为了逼近某个数据分布，比如预测就是一个分类问题；强化学习就是一种决策型的任务。模式识别与机器学习在许多算法上是通用的，但模式识别的目的在于使计算机具备识别能力，机器学习的目的在于使计算机更好地理解环境并与环境交互。

### 系统组成

![image-20250313222101151](../assets/post-pics/image-20250313222101151.png)

设计一个模式识别系统，就是要设计分类器的模型、所使用的特征和分类器参数的调整算法。

### 基本概念

将模式信息提取出特征，并将每一个特征作为一个维度构成一个多维特征空间。数值型特征构成向量空间，非数值特征构成集合空间。样本之间的相似程度用特征空间中点的相似程度来计算，每一类样本的聚集区域则表现为特征空间中点的统计分布。数值型特征的相似度度量标准有距离、余弦相似度、皮尔逊相关系数、JACCARD 相似系数等；非数值型距离度量有编辑距离等。

同一类样本点的相似程度用紧致性来衡量，紧致性准则要求样本集中属于同一个类的样本间的相似度，应当远大于属于不同类的样本间的相似度。增加特征空间的维数或进行空间映射变换，是增强该问题模式类的紧致性的常用方法。类别可分型度量是紧致性的量化表示，常用的有基于类内类间距离和基于概率距离两种。当数据维度增加，训练集样本的数量不足而变得稀疏，样本集的紧致性变差，会导致算法性能急剧下降，这被称为维数灾难。

分类器通过某些算法找到自身最优参数的过程，称为分类器的训练或学习。学习分为有监督学习、无监督学习、半监督学习和自监督学习。采用无监督学习的分类器，能够达到更高的智能水平，也是未来模式识别技术发展的主要方向，半监督和自监督是灵活运用有监督和无监督的学习方式。

训练好的分类器对未知新样本正确分类的能力称为泛化能力，而训练好的分类器对未知新样本正确分类的能力大幅度下降的现象称为过拟合，过拟合的来源是误差数据，而误差数据是无法避免的。

## 第二章 特征工程

特征生成和特征降维统称为特征工程。各个维度的特征对于分类的贡献不一，因此可以进行特征降维以删除冗余信息、减少算法计算量和对样本集规模的需求，保留对分类最有效的信息。特征降维的方法主要包括特征选择和特征提取。 

特征工程的方法有数据预处理（如数据去重、缺值补充、数据增强、异常值处理等）、特征提取（通过某种变换，将原始特征从高维空间映射到低维空间）、特征选择（从高维特征中挑选出一些最有效的特征，如独立算法、穷举算法、分支界定法、顺序前进/后退、动态顺序前进等）、维度压缩（如PCA等）、特征构造、特征变换（如标准化、归一化、对数变换等）、特征编码（如标签编码、独热编码、隶属度函数等）、特征融合。

特征工程的方法需要人工干预，缺少通用的流程和方法。与之对应的端到端技术，就是指跳过特征生成和特征降维环节，直接将具有原始特征的样本数据输入深度神经网络模型，由模型在学习过程中自行提取出对分类最有效的特征，并应用于最终的分类决策。

## 第三章 统计学习理论

机器学习的基本问题可以描述为，根据给定的已知训练样本求取对系统的输出与输入之间依赖关系的估计， 使它能对给定输入下的未知输出做出尽可能准确的预测，即：根据 n 个独立同分布观测样本 $$(x_1,y_1),(x_2,y_2),\cdots,(x_n,y_n)$$，在一组函数$$\{f(x, w)\}$$中寻求最优的一个函数$$\{f(x, w_0)\}$$对依赖关系进行估计，使期望风险最小

$$
R(w)=\int L(y,f(\boldsymbol{x},w))dF(\boldsymbol{x},y) \quad L(y,f(\boldsymbol{x},w))为预测造成的损失，称为损失函数
$$

机器学习不同问题下损失函数的定义如下

- 有监督学习的分类问题，系统输出为类别标签，以二分类为例

  $$
  \begin{aligned}
  L(y,f(\boldsymbol{x},w))=
  \begin{cases}
  0, & y = f(\boldsymbol{x},w)\\
  1, & y \neq f(\boldsymbol{x},w)
  \end{cases}
  \end{aligned}
  $$

  此时的期望风险就是贝叶斯决策中使错误率最小

- 函数拟合中，误差采用最小平方误差

  $$
  L(y,f(\boldsymbol{x},w))=(y - f(\boldsymbol{x},w))^2
  $$

- 对于概率密度估计，目的是确定概率密度函数$$p(x, w)$$
  $$
  L(p(\boldsymbol{x},w))=-\log p(\boldsymbol{x},w)
  $$

### 函数的容量和 VC 维

函数集在一组样本集上可能实现的分类规则数目称为函数集的容量。VC维的直观定义，对一个指示函数，如果存在$$k$$个样本能够被函数集中的函数按所有可能的$$2^k$$种形式分开，则称函数集能把样本数为$$k$$的样本集打散。指示函数集的 VC 维就是能够打散的最大样本集的样本数目。如果对于任意数目的样本，总能找到一个样本集能够被这个函数集打散，则函数集的 VC 维就是无穷大。函数集的 VC 维并不简单地与函数中自由参数个数有关，而是与函数本身的复杂程度有关。

### 经验风险最小化原则

机器学习就是在函数集中最小化期望风险的期望风险泛函，但这个风险泛函需要对服从联合概率密度$$F(x, y)$$的所有可能样本及其输出值求期望，这在$$F(x, y)$$未知的情况下无法进行。根据大数定律，用算数平均值代替期望，定义经验风险为在训练样本上损失函数的平均，传统的机器学习方法都用这中方式来代替最小化期望风险的目标，这种策略称为经验风险最小化原则（ERM 原则）。

当训练样本数趋向于无穷大时，以经验风险最小化原则进行的学习与期望风险最小的目标是否一致，这称为学习过程的一致性。学习理论关键定理指出，对于有界的损失函数，经验风险最小化学习一致的充分必要条件是，经验风险在如下式意义上一致地收敛于真实风险
$$
\lim_{n\rightarrow\infty}P\left\{\sup_{w_n}\left(R(w_n)-R_{EMP}(w_n)\right)>\varepsilon\right\}=0, \quad \forall\varepsilon > 0
$$

### 结构风险最小化原则

学习机器的实际风险由经验风险（训练误差）和置信范围组成，置信范围和学习机器的VC维及训练样本量有关。样本数量较少时置信范围较大，用经验风险最小化取得的最优解可能会有较大的期望风险，即可能推广性较差，因此需要同时最小化经验风险和置信范围，而不是单独最小化经验风险。通过按照VC维的大小排序并划分函数子集结构，学习目标化为在函数集中同时进行子集的选择和子集中最优函数的选择。选择最小经验风险与置信范围之和最小的子集，就可以达到期望风险的最小，这个子集中使经验风险最小的函数就是要求的最优函数。这种策略称作结构风险最小化原则（SRM 原则）。

对于样本数给定的问题，学习机器的VC维越高（复杂性越高），置信范围就越大，真实风险与经验风险之间可能的差别越大，推广能力可能越差。因此对于有限样本的任务，应该尽可能选用相对简单的分类器来期望获得尽可能好的推广能力。 

### 正则化方法

正则化是解决不适定问题的一类方法（不适定问题是指，即使方程存在唯一解，方程右边的微小扰动$$\vert F - F_{\delta} \vert < \delta$$会带来很大变化；后来发现应该最小化以下的正则化泛函：
$$
R^*(f)=\|Af - F_{\delta}\|^2+\lambda(\delta)\Omega(f)
$$
对于机器学习问题大部分都是用样本特征和对应的观测数据来拟合它们之间的函数关系，所以都属于不适定问题。为解决这个问题，在机器学习模型学习过程中 就必须要引入正则化方法，通常是在模型的损失函数中添加一个正则项（惩罚项）来实现。常见的正则化方法有：Lasso正则化（L1正则化）、Tikhonov正则化（L2正则化）、弹性网正则化（混合正则化）；对于正则化方式使用的范数，也是一种超参数。

## 第四章 线性分类器

模式识别的决策论方法是指，以一定的规则集将特征空间划分为不同的决策域，决策域包含了样本的分布区域但不等同，判定样本所属的决策域就是分类。判别函数是参数集的函数，是决策边界的方程表示，这个参数集就是权向量，找判别函数就是极小化准则函数的泛函问题。满足判别函数的某个条件则说明属于这一类，否则说明不属于；显然，在二分类问题中，不属于该类则属于另一类。

### 前置知识

#### 线性判别函数

$$
\begin{aligned}
G(\boldsymbol{x}) &= \boldsymbol{w}^T\boldsymbol{x} + w_0\\
\boldsymbol{x}&=[x_1, x_2, \dots, x_n]^T \\
\boldsymbol{w}&=[w_1, w_2, \dots, w_n]^T\\
\end{aligned}
$$

也可写成增广形式，将$$w_0$$加入权向量中，本质相同，处理上简单一些。线性判别函数为一个超平面，权向量$$\boldsymbol{w}$$与该超平面正交，$$\frac{\vert w_0 \vert}{\vert\vert \boldsymbol{w} \vert\vert}$$为超平面到原点距离，判别函数是样本到超平面距离的一种度量。

#### 多分类问题

- 绝对可分：有许多分类不确定的区域，$$m$$个类有$$m$$个判别函数；应用最广泛
- 两两可分：分类不确定区域减少，$$m$$个类有$$\frac{m(m-1)}{2}$$个判别函数
- 最大值可分：样本在每个类的判别函数都有值，归到取值最大的判别函数对应的类中，是两两可分的特殊情况

#### 广义线性化

两个类别线性可分的条件为：分布范围本身是互不重叠的全连通单一凸集。某些线性不可分的问题可通过映射到另一个高维空间转化为线性可分问题，这称为广义线性化。

#### 线性分类器原理

设计：设决策边界方程如下（以增广形式为例），若问题线性可分则找最优解即可，若非线性可分则问题无解
$$
\begin{aligned}
G_j(\boldsymbol{x})&=\boldsymbol{W}^T\boldsymbol{X}
\end{aligned}
$$
训练：设定一个标量函数$$J(\boldsymbol{w})$$作为准则函数，求解使得准则函数取极小值的权向量$$\boldsymbol{w}$$就是最优解。一般来说无法通过$$\frac{\partial J(\boldsymbol{w})}{\partial \boldsymbol{w}}=0$$直接获得解析解，而是通过逐步逼近的算法根据训练集求取最优的权向量。最常见的逐步逼近算法为梯度下降法，即从初始权向量$$\boldsymbol{w}(0)$$出发，每次沿当前$$J(\boldsymbol{w})$$的负梯度方向前进一步来修正权向量
$$
\begin{aligned}
\boldsymbol{w}(k + 1) &=\boldsymbol{w}(k)-\rho(k + 1)\nabla J(\boldsymbol{w}(k))
\end{aligned}
$$
其中$$\rho(k+1)$$为第$$K+1$$步的步长，又称学习率，是一种超参数，可以给固定值，也可以通过绝对修正、部分修正、最优步长等方式随学习过程变化，较为常用的策略是令$$\rho(k+1) = \frac{\lambda}{k}$$

### 感知器算法





## 临时记录

Linear Algebra Done Right 及其练习

泛函问题

$$X^\dagger$$

权向量归一化后，超平面的位置由 w_0 决定

线性不可分的条件，凸包，连通性

广义线性化：如果能找到非线性边界，想办法映射到高维，扩维使其线性。支持向量机在理论上就是用的广义线性化

线性分类器本质上只能进行二分类，多分类本质上是多个二分类的组合。绝对可分，多分类组合的不可识别区域非常大。两两可分的不可识别区域相对较小。最大值可分，要看哪一类的值最大，是一种特殊的两两可分。目前支持向量机处理多分类问题仍然采用绝对可分。

判别函数的值代表了距离分类决策边界的距离

训练分类器的一般思路：设定准则函数、求极小值，则求得最优值；改变权向量来优化

变换：增广、规范化

具体求解的算法：感知器算法，最大特点是把生物神经元机理做了数学抽象，把树突收集到的信号与激活阈值进行比较，大于则产生输出。准则函数设为错分样本的集合。用梯度下降法求取，有批量梯度下降法和随机梯度下降法（课程中其实应该叫单词梯度下降法，要把每个样本都用来下降一次）

学习速率设定：固定式、绝对修正、部分修正、变速学习（用的多）、优化学习（把步长也最优化）。下降受样本集影响，不一定都能收敛

线性代数研究的是向量空间及其变换，压缩

关联最密切的是协方差矩阵，特征值分解

泛函：最优化控制里面必讲泛函

核心是首先由函数空间，无穷维的空间，每个向量是一个函数，复返空间

值域在普通的向量空间，泛函就是无穷维空间到这个向量空间的映射，函数空间

每一个函数对应一个实数，研究函数取什么形式，可以使得函数取某一个值，类似于最优化问题

研究函数取什么形式，函数能取得这个值，模型求解就是求泛函的问题。求取什么参数时，准则函数可以求得极值，通过算子求得极值

求求解这个泛函的方法

手写数字识别的流程

![image-20250318163411713](../assets/post-pics/image-20250318163411713.png)
