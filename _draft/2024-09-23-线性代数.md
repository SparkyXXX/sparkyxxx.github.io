---
title: 线性代数
description: 课程笔记
author: Hatrix
date: 2024-09-23 10:08:00 +0800
categories: [基础数学]
tags: [课程笔记]
math: true
mermaid: true
pin: false
---

线性代数中有许多从不同角度叙述，但本质相同的东西。

协方差矩阵：协方差用于衡量两个随机变量之间的线性关系，正值表示变化趋势相同，负值表示变化趋势相反，零值表示不存在线性关系；协方差大小受量纲影响，使用之前需要做归一化。两个以上随机变量则用协方差矩阵表示，将各个变量之间的协方差填充到矩阵中，构成一个对称矩阵。使用中心化(零均值)后的数据矩阵$$\mathbf{X}_\text{centered}$$，协方差矩阵$$\mathbf{C} = \frac{1}{n-1}\mathbf{X}_\text{centered}^T\mathbf{X}_\text{centered}$$。

## Brant 的线性代数

矩阵乘以列向量有两种理解方式，一种是解线性方程组的角度，将矩阵按行看，一行代表一个线性方程；另一种是向量线性组合的角度，一列代表一个基向量(后一种更舒服一些)。行向量乘以矩阵理解为矩阵的行的线性组合，由此可以表示出高斯消元法对应的初等矩阵，所有初等变换矩阵的乘积是一个下三角矩阵，消元之后的矩阵是一个上三角矩阵，即高斯消元法可以将非奇异矩阵分解为下三角矩阵与上三角矩阵的乘积。

两个矩阵相乘，如果将左边矩阵看作被操作的对象，右边矩阵看作操作，称为右乘，右乘是对矩阵的列进行线性组合；如果将右边矩阵看作被操作的对象，左边矩阵看作操作，称为左乘，左乘是对矩阵的行进行线性组合。

矩阵乘法 AB=C，可以看作 A 乘 B 的各列，即 A 各列的线性组合；也可看作 A 的各行乘 B，即 B 各行的线性组合。

矩阵各列的线性组合称为矩阵的列空间，矩阵各行的线性组合称为矩阵的行空间。

矩阵乘法可以分块

对于非奇异的方阵，左逆等于右逆，非方阵有伪逆，左伪逆不等于右伪逆。

对称矩阵是很普遍的矩阵，任何矩阵乘以其转置(或转置乘以本身)，得到的都是对称矩阵。

高斯若尔当方法求逆，拼一个单位阵进行消元

矩阵求逆提供了一种同时处理多个方程组的思路

消元矩阵(这里指向下消元和向上消元都进行)就是原矩阵的逆

lu 分解将矩阵分解为下三角和上三角，可以降低算法的计算复杂度

置换矩阵是使得矩阵两行(或两列)互换的操作矩阵，以三维方阵为例，置换矩阵一共六个(算上单位阵)(n 阶有 n 的阶乘个)，它们对乘法封闭，逆就是转置(四个非轮换的本身是对称矩阵，另外两个互为转置)(正交矩阵)

向量空间的定义，子空间的定义

子空间举例，三维空间中任何一个过原点的平面、过原点的直线、原点本身，都是 R3 的子空间

子空间的并集不是子空间，交集是子空间

矩阵的行或列的线性组合可以产生子空间，比如 3\*2 矩阵的两列的线性组合构成子空间，列属于 R3，那么列的线性组合就是 R3 的子空间

Ax = b 有解的条件时，当且仅当 b 属于 A 的列空间，即 b 是 A 各列的线性组合

满足 Ax=0 的 x，构成矩阵 A 的零空间；零空间是针对线性映射的概念，描述了输入侧的向量空间中的哪些元素，经过映射/变换之后坍缩成了零元素，这些输入侧元素构成的向量空间称为零空间。显然，零空间是输入侧向量空间的子空间，因为零空间的元素全部来自输入侧向量空间。

假设 A 为 m\*n 的矩阵，A 的列空间是 m 维的空间，A 的零空间是 n 维的空间；向量空间的维数，是这个空间中基向量的数量。

消元之后，主元所在的列对应的系数称为主变量，其它列对应的系数称为自由变量。向上消元和向下消元再将主元都化为 1 之后，得到的矩阵称为简化行阶梯型形式。

主元数量称为矩阵的秩 rank，以列的角度看，假设矩阵为 m\*n，那么自由变量有 n-r 个。

给自由变量赋值 0 和 1 可以得到特解，特解的线性组合构成零空间(Ax=b 的所有解)。

零空间矩阵是将所有特解作为列的矩阵

最简化的行阶梯形式如下

$$
\mathbf{R} =
\begin{bmatrix}
\mathbf{I} & \mathbf{F}\\
\mathbf{0} & \mathbf{0}\\
\end{bmatrix}
$$

构造零空间矩阵$$\mathbf{N}$$(所有特解作为列构成的矩阵)，即$$\mathbf{R}\mathbf{N} = \mathbf{0}$$

$$
\mathbf{N} =
\begin{bmatrix}
\mathbf{-F}\\
\mathbf{I}\\
\end{bmatrix}
$$

由

$$
\mathbf{RX} = \mathbf{0}\\
\begin{bmatrix}
\mathbf{I} & \mathbf{F}\\
\end{bmatrix}
\begin{bmatrix}
\mathbf{x_{pivot}} \\
\mathbf{x_{free}}\\
\end{bmatrix} = \mathbf{0}\\
\mathbf{x_{pivot}} = -\mathbf{F}\mathbf{x_{free}}
$$

列满秩(r=n)时，如果解存在，那么解唯一；行满秩(r=m)总有解。自由变量的个数为 n-r，行满秩时这个数量为 n-m，这部分构成零空间的特殊解。

Ax=b 的所有解表示为特解$$x_p$$加上零空间中任意向量$$x_n$$($$x_n$$中是带常数 c 的)。两个解的差一定落在零空间内，因此特解加上这个差就可以表示任意的解。另外，特解又称为基础解系

$$
\begin{aligned}
\mathbf{A}\mathbf{x_p} = \mathbf{b}\\
\mathbf{A}\mathbf{x_n} = \mathbf{0}\\
\mathbf{A}(\mathbf{x_p} + \mathbf{x_n}) = \mathbf{b}\\
\end{aligned}
$$

![](https://raw.githubusercontent.com/SparkyXXX/Hatrix-s-Blog-Image/refs/heads/main/img/20250928165616870.png)

![](https://raw.githubusercontent.com/SparkyXXX/Hatrix-s-Blog-Image/refs/heads/main/img/20250928165445790.png)

既然是这样，那解空间是啥？

矩阵的秩决定了方程组解的数目。

线性无关的定义：一组向量线性组合的系数只有全为零才能使得线性组合的结果为零。

基：既能生成向量空间，本身又是线性无关的。

空间$$R^n$$的基向量个数为 n，一个向量空间可以有无数组基，但同一个向量空间所有基包含的向量个数是相同的，这个数量称为向量空间的维数。列空间的维数是秩。零空间的维数是自由变量的数目。描述一个向量空间的方式就是找到空间的一组基，然后确定它的维数，

列变换不影响列空间，但会影响行空间，行变换类似。

组成矩阵的各列向量之间线性无关的等价条件是，矩阵可逆。(列向量之间线性无关，等价于列向量们可以作为列空间的基)

对于 m\*n 矩阵而言，行空间和零空间位于$$R^n$$，它们的维数加起来为 n，列空间和左零空间(转置的零空间)位于$$R^m$$，它们的维数加起来为 m

向量空间是个抽象的概念，例如，所有 3\*3 的矩阵可以构成向量空间，因为这些矩阵完全满足向量空间这种代数结构的定义，具有向量空间的所有性质，同样有基、维数这些概念。上三角矩阵和对称矩阵是其中的两个子空间，它们的交集是对角矩阵，显然也是子空间。再例如，对于微分方程$$y''+y=0$$，它的解也构成向量空间，显然，一组基就是 sinx 和 cosx。

秩一矩阵可以表示为一列乘以一行的形式，秩一矩阵是一种基本单元的存在，例如秩为 4 的矩阵，可以通过四个秩一矩阵搭建出来。

矩阵可以用来编码图和网络，关联矩阵源自于问题，描述了问题的拓扑结构，比如电路连接、建筑结构、人际关系网等等。

对于 m\*n，秩为 r 的矩阵来说，行空间和零空间都是$$R^n$$的子空间，列空间和左零空间都是$$R^m$$的子空间；行空间和列空间的维度都是 r，零空间的维度是 n-r，左零空间的维度是 m-r。零空间的基就是特解，特解通过给某一个自由变量赋值 1 其它的自由变量赋值 0，依次对所有自由变量操作得到，自由变量的个数为 n-r，零空间基中向量个数也是 n-r，因此零空间维度为 n-r。行空间和列空间的基分别为主元对应的行和主元对应的列。

行空间和零空间是正交的，列空间和左零空间是正交的。以行空间和零空间为例，它们都是$$R^n$$的子空间，可以想象把整个$$R^n$$划分为两个相互正交的部分，例如把$$R^3$$划分为过原点的平面以及过原点并垂直于平面的直线。向量正交等价于点积为 0；向量空间正交意味着向量空间中任意两个向量都正交，这一点意味着两个正交的向量空间不能有非零向量以外的交集。

$$\mathbf{A}^\top \mathbf{A}$$可逆当且仅当$$\mathbf{A}$$的各列线性无关

投影矩阵如下，投影矩阵满足$$\mathbf{P}^\top = \mathbf{P}$$，$$\mathbf{P}^2 = \mathbf{P}$$

$$
\mathbf{P} = \frac{\mathbf{A}\mathbf{A}^\top}{\mathbf{A}^\top \mathbf{A}} \quad \text{向量}\\
\mathbf{P} = \mathbf{A} (\mathbf{A}^\top \mathbf{A})^{-1} \mathbf{A}^\top \quad \text{向量}\\
$$

投影的意义是，AX = b 无解，因此将 b 投影到 A 的列空间中得到 p，解 AX = p 方程。两个极端情况是，如果 b 在 A 的列空间中，则 Pb = b；如果 b 和 A 的列空间正交，则 Pb = 0。

最好的基是标准正交基，以正交基为列或行可构成正交矩阵。

正交矩阵$$\mathbf{Q}$$满足$$\mathbf{Q}^\top = \mathbf{Q}^{-1}$$。显然正交矩阵必须是方阵。

施密特正交化是一种将任意基转化为标准正交基的方法，转化前后的基所张成的向量空间保持不变。施密特正交化方法，实质上就是将向量分解为在既有向量张成的向量空间中的分量，以及垂直于这个向量空间的向量分量两个分量，然后只取垂直分量，保证和之前的向量组均正交。最后归一化即可。

用矩阵表示施密特正交化：A = QR，其中 R 为上三角矩阵，Q 为正交矩阵。

使用正交基来构成矩阵的好处是方便求逆，并简化投影矩阵的计算。使得$$x_i = q_i^\top b$$，这也就是将向量正交化的意义。

行列式为 0 时，矩阵是奇异的；交换行或列奇数次，行列式的符号取反

行列式描述了对向量空间的拉伸或压缩程度，在二维或三维的情况下表示有向面积或有向体积。某一行或某一列乘以一个系数，行列式的结果也乘以这个系数

三角矩阵的行列式等于对角线元素之积

$$
\text{det}(\mathbf{A}^2) = (\text{det}\mathbf{A})^2\\
\text{det}(2\mathbf{A}) = 2^n\text{det}\mathbf{A}\\
$$

二阶矩阵行列式公式

$$
\begin{vmatrix}
a_{11} & a_{12} \\
a_{21} & a_{22}
\end{vmatrix} = a_{11}a_{22} - a_{12}a_{21}
$$

二阶矩阵求逆

$$
\mathbf{A}^{-1} = \frac{1}{\det(\mathbf{A})}
\begin{pmatrix}
a_{22} & -a_{12} \\
-a_{21} & a_{11}
\end{pmatrix}
\quad \text{其中} \quad
\det(\mathbf{A}) =
\begin{vmatrix}
a_{11} & a_{12} \\
a_{21} & a_{22}
\end{vmatrix} = a_{11}a_{22} - a_{12}a_{21} \neq 0
$$

行列式性质

$$
% 行列式的基本性质
\begin{align*}
&1. \quad \begin{vmatrix} a_{11} & a_{12} & \cdots & a_{1n} \\ \vdots & \vdots & & \vdots \\ a_{i1} & a_{i2} & \cdots & a_{in} \\ \vdots & \vdots & & \vdots \\ a_{n1} & a_{n2} & \cdots & a_{nn} \end{vmatrix}
= \begin{vmatrix} a_{11} & a_{21} & \cdots & a_{n1} \\ a_{12} & a_{22} & \cdots & a_{n2} \\ \vdots & \vdots & & \vdots \\ a_{1n} & a_{2n} & \cdots & a_{nn} \end{vmatrix} \quad (\text{转置行列式相等}) \\[6pt]

&2. \quad \begin{vmatrix} a_{11} & a_{12} & \cdots & a_{1n} \\ \vdots & \vdots & & \vdots \\ ka_{i1} & ka_{i2} & \cdots & ka_{in} \\ \vdots & \vdots & & \vdots \\ a_{n1} & a_{n2} & \cdots & a_{nn} \end{vmatrix}
= k \begin{vmatrix} a_{11} & a_{12} & \cdots & a_{1n} \\ \vdots & \vdots & & \vdots \\ a_{i1} & a_{i2} & \cdots & a_{in} \\ \vdots & \vdots & & \vdots \\ a_{n1} & a_{n2} & \cdots & a_{nn} \end{vmatrix} \quad (\text{某行乘常数等于行列式乘该常数}) \\[6pt]

&3. \quad \begin{vmatrix} a_{11} & a_{12} & \cdots & a_{1n} \\ \vdots & \vdots & & \vdots \\ a_{i1}+b_{i1} & a_{i2}+b_{i2} & \cdots & a_{in}+b_{in} \\ \vdots & \vdots & & \vdots \\ a_{n1} & a_{n2} & \cdots & a_{nn} \end{vmatrix}
= \begin{vmatrix} a_{11} & a_{12} & \cdots & a_{1n} \\ \vdots & \vdots & & \vdots \\ a_{i1} & a_{i2} & \cdots & a_{in} \\ \vdots & \vdots & & \vdots \\ a_{n1} & a_{n2} & \cdots & a_{nn} \end{vmatrix} + \begin{vmatrix} a_{11} & a_{12} & \cdots & a_{1n} \\ \vdots & \vdots & & \vdots \\ b_{i1} & b_{i2} & \cdots & b_{in} \\ \vdots & \vdots & & \vdots \\ a_{n1} & a_{n2} & \cdots & a_{nn} \end{vmatrix} \quad (\text{某行拆分为两行之和，行列式也拆分}) \\[6pt]

&4. \quad \begin{vmatrix} a_{11} & a_{12} & \cdots & a_{1n} \\ \vdots & \vdots & & \vdots \\ a_{i1} & a_{i2} & \cdots & a_{in} \\ \vdots & \vdots & & \vdots \\ a_{j1} & a_{j2} & \cdots & a_{jn} \\ \vdots & \vdots & & \vdots \\ a_{n1} & a_{n2} & \cdots & a_{nn} \end{vmatrix}
= -\begin{vmatrix} a_{11} & a_{12} & \cdots & a_{1n} \\ \vdots & \vdots & & \vdots \\ a_{j1} & a_{j2} & \cdots & a_{jn} \\ \vdots & \vdots & & \vdots \\ a_{i1} & a_{i2} & \cdots & a_{in} \\ \vdots & \vdots & & \vdots \\ a_{n1} & a_{n2} & \cdots & a_{nn} \end{vmatrix} \quad (\text{交换两行，行列式变号}) \\[6pt]

&5. \quad \text{若行列式两行完全相同，则行列式值为 } 0 \quad (\text{由性质4推论}) \\[6pt]

&6. \quad \begin{vmatrix} a_{11} & a_{12} & \cdots & a_{1n} \\ \vdots & \vdots & & \vdots \\ a_{i1} & a_{i2} & \cdots & a_{in} \\ \vdots & \vdots & & \vdots \\ ka_{i1}+a_{j1} & ka_{i2}+a_{j2} & \cdots & ka_{in}+a_{jn} \\ \vdots & \vdots & & \vdots \\ a_{n1} & a_{n2} & \cdots & a_{nn} \end{vmatrix}
= \begin{vmatrix} a_{11} & a_{12} & \cdots & a_{1n} \\ \vdots & \vdots & & \vdots \\ a_{i1} & a_{i2} & \cdots & a_{in} \\ \vdots & \vdots & & \vdots \\ a_{j1} & a_{j2} & \cdots & a_{jn} \\ \vdots & \vdots & & \vdots \\ a_{n1} & a_{n2} & \cdots & a_{nn} \end{vmatrix} \quad (\text{某行乘k加到另一行，行列式值不变}) \\[6pt]

&7. \quad \text{若行列式某行元素全为0，则行列式值为 } 0 \\[6pt]

&8. \quad \text{若行列式两行成比例，则行列式值为 } 0 \quad (\text{由性质2和5推论})
\end{align*}
$$

行列式的计算公式，利用代数余子式的概念将高阶行列式展开为低阶行列式；代数余子式就是结合逆序数的概念，由左上角元素构成的方阵。

## Linear Algebra Done Right

向量空间的定义：带有加和数乘两种操作并满足这两种操作的一些特殊性质的集合，这样一种代数结构。$$\boldsymbol{F}^S$$是从集合$$S$$到$$\boldsymbol{F}$$域的函数集合，也是向量空间的一种。

原集合的子集在满足一定条件(存在零元素/加法单位元、对加法和数乘封闭)下，称为子空间。子空间可以求和，求和后表示唯一则为直和。判断和是否为直和有一些方法。

一组向量可以线性组合，一组向量的所有线性组合的集合称为这组向量张成的空间。如果一个向量空间可以被其中的一组向量张成，则称为有限维的向量空间。多项式是一个有限维的向量空间。张成空间中的向量的表示方式唯一(也等价于零元素的表示方式唯一，即系数全零)，则张成这个空间的向量称为线性无关。线性无关并能张成向量空间的一组向量称为这个向量空间的基，基中向量的个数定义为向量空间的维度。

从 chapter 3 开始：

线性映射定义为满足以下两条性质的函数

![image-20250929000900861](https://raw.githubusercontent.com/SparkyXXX/Hatrix-s-Blog-Image/refs/heads/main/img/image-20250929000900861.png)

两组基之间的线性映射有且仅有一个。

经过线性映射之后的元素构成的集合称为线性映射的值域 range，显然，值域是输出侧向量空间的子空间。

线性映射有单射、满射和双射的概念。单射等价于零空间只包含零元素，满射等价于值域等于输出侧向量空间。双射即同时满足单射和满射。高维向低维映射不可能是单射，低维向高维映射不可能是满射。

可以定义线性映射的加法、数乘、乘法运算，这些运算也满足交换律、分配律这样的性质，也存在幺元这样的元素，线性映射本身也是一种向量空间，更明确的说，是从一个向量空间到另一个向量空间的所有线性映射构成的集合，可以构成向量空间这样的代数结构。

线性映射基本定理：零空间的维数(nullity)+ 值域的维数(rank)= 输入空间的维数.

![](https://raw.githubusercontent.com/SparkyXXX/Hatrix-s-Blog-Image/refs/heads/main/img/20250928185353147.png)

这个定理就像在 “丈量” 线性映射对向量空间的 “压缩” 与 “保留” 程度：零空间的维数反映了有多少维度的信息在映射中 “丢失”(被压缩到零向量)，值域的维数反映了有多少维度的信息被 “保留”(映射后形成新的向量空间)，而定义域空间的维数就是这两部分的总和。

- 零空间告诉你：多少维的输入会被压缩成零。
- 值域告诉你：输出空间里能“占据”多少维的方向。
- 两者相加 = 输入空间的总维数。

这其实就是**“信息守恒”**：输入空间每一个维度，不是被“杀掉”就是被“保留下来”，没有别的可能。

至此，终于可以引出矩阵的概念了，矩阵可以用来编码线性映射，把线性映射中处理每个基向量的这些系数组织在一起，就构成了线性映射对应的矩阵。线性映射可以通过矩阵乘法来描述。

矩阵同样可以定义加法、数乘、乘法运算，并且满足相应的性质，矩阵也是一种向量空间。(矩阵本身是变换的一种记法，自然也是一种向量空间)

线性映射可以定义可逆性，逆映射是唯一的。映射可逆等价于映射是双射。可逆变换又称为同构，两个向量空间如果存在可逆变换，那么称他们是同构的。同构等价于两个向量空间的维数相同。

线性映射的维数等于映射前后向量空间的维数之积。

![image-20250929010843237](https://raw.githubusercontent.com/SparkyXXX/Hatrix-s-Blog-Image/refs/heads/main/img/image-20250929010843237.png)

维数是基向量的个数，对于线性映射这个向量空间来说，基向量是输入映射到输出的函数关系，例如

![image-20250929011125235](https://raw.githubusercontent.com/SparkyXXX/Hatrix-s-Blog-Image/refs/heads/main/img/image-20250929011125235.png)

矩阵和向量其实都是在省略了基的情况下的一种记法，记录了某个元素在给定基的情况下的组合系数。

在有限维向量空间中，可逆、单射和满射是等价的。

函数内积

特征向量难道是一组基？

怎么理解行列式等于主元之积

## 第一章 向量空间

线性代数用矩阵和向量的角度组织信息。向量即一维数组，有加减乘除、内积外积的基本运算。

向量之间有线性相关和线性无关的概念。若一组向量$$\{\boldsymbol{v_{1}}, \boldsymbol{v_{2}}, \ldots, \boldsymbol{v_{n}}\}$$的线性组合等于零向量的唯一解，是所有系数均为零时，称这个向量组是线性无关的；否则称为线性相关。

$$
c_{1} \boldsymbol{v_{1}} + c_{2} \boldsymbol{v_{2}} + \cdots + c_{n} \boldsymbol{v_{n}} = \boldsymbol{0}
$$

一个线性无关的向量组，若在此组中添加任何一个新的向量都会使得向量组变为线性相关时，这个向量组称为极大无关组；极大无关组中的向量个数称为极大无关组的秩，也是其构成向量空间的维度；维度表示了生成线性空间所需要的最少的向量个数。秩有许多不同角度的定义，是一个具有代表性的特征，有某种不变性。极大无关组中的所有向量构成该向量组的一个基。用向量组的基可以表示其它向量，表示的系数称为坐标。

一个向量集合，若对加和数乘两种运算封闭，则称为向量空间。用群论的角度看，向量空间的数学结构是一个群。向量空间的子集称为子空间。一个向量组中向量的所有线性组合称为这个向量组的张成空间。广义的向量空间，其中的元素不局限于向量，如函数空间。

## 第二章 矩阵

在解线性方程组时，发现未知数的形式并不重要，因此根据方程组抽象出了系数矩阵和增广矩阵，这便引出了矩阵。矩阵是一个按矩形阵列排列的数值集合，代表了对空间的一种线性变换，也可以代表一个高维空间，是用于处理多维数据的重要数学结构。用矩阵各列/行构成向量，张成的线性空间叫列/行空间；线性变换后被压缩到原点的向量构成零空间。

### 2.1 矩阵运算

设两个矩阵$$\boldsymbol{A}、\boldsymbol{B}$$，标量$$c$$

#### 加和数乘

矩阵满足普遍意义上的加和数乘

$$
\boldsymbol{A} = \begin{pmatrix} a_{11} & a_{12} \\ a_{21} & a_{22} \end{pmatrix}, \quad \boldsymbol{B} = \begin{pmatrix} b_{11} & b_{12} \\ b_{21} & b_{22} \end{pmatrix}
$$

$$
\boldsymbol{C} = \boldsymbol{A} + \boldsymbol{B} = \begin{pmatrix} a_{11} + b_{11} & a_{12} + b_{12} \\ a_{21} + b_{21} & a_{22} + b_{22} \end{pmatrix} \\ c\boldsymbol{A} = \begin{pmatrix} ca_{11} & ca_{12} \\ ca_{21} & ca_{22} \end{pmatrix}
$$

#### 乘法

$$
\boldsymbol{A} = \begin{pmatrix} a_{11} & a_{12} & \cdots & a_{1n} \\ a_{21} & a_{22} & \cdots & a_{2n} \\ \vdots & \vdots & \ddots & \vdots \\ a_{m1} & a_{m2} & \cdots & a_{mn} \end{pmatrix}_{m \times n}, \quad \boldsymbol{B} = \begin{pmatrix} b_{11} & b_{12} & \cdots & b_{1k} \\ b_{21} & b_{22} & \cdots & b_{2k} \\ \vdots & \vdots & \ddots & \vdots \\ b_{n1} & b_{n2} & \cdots & b_{nk} \end{pmatrix}_{n \times k}
$$

$$
\boldsymbol{C} = \boldsymbol{A} \boldsymbol{B} = \begin{pmatrix} c_{11} & c_{12} & \cdots & c_{1k} \\ c_{21} & c_{22} & \cdots & c_{2k} \\ \vdots & \vdots & \ddots & \vdots \\ c_{m1} & c_{m2} & \cdots & c_{mk} \end{pmatrix}_{m \times k}
$$

其中$$C$$的第$$i$$行第$$j$$列的元素为$$A$$中第$$i$$行和$$B$$的第$$j$$列作内积得到

$$
c_{ij} = a_{i1}b_{1j} + a_{i2}b_{2j} + \cdots + a_{in}b_{nj}
$$

注意，矩阵乘法要求维度匹配，只有当第一个矩阵的列数等于第二个矩阵的行数时才能进行乘法运算。在此例中，$$A$$ 的形状是$$m \times n$$，$$B$$的形状是$$n \times k$$，结果为$$m \times k$$的矩阵。同时，由于对维度的要求，乘法不满足数值乘法所满足的交换律，因此需要区分左乘和右乘。

对矩阵进行乘法，表示对空间进行变换；左乘和右乘分别代表对列空间和行空间的空间变换，是两种对偶的研究角度。

#### 转置

$$
\boldsymbol{A} = \begin{pmatrix}a_{11} & a_{12} & \cdots & a_{1n} \\a_{21} & a_{22} & \cdots & a_{2n} \\\vdots & \vdots & \ddots & \vdots \\a_{m1} & a_{m2} & \cdots & a_{mn}\end{pmatrix}
$$

$$
\boldsymbol{A^{\text{T}}} = \begin{pmatrix}a_{11} & a_{21} & \cdots & a_{m1} \\a_{12} & a_{22} & \cdots & a_{m2} \\\vdots & \vdots & \ddots & \vdots \\a_{1n} & a_{2n} & \cdots & a_{mn}\end{pmatrix}
$$

转置就是沿对角线调换元素位置，满足以下性质

$$
\begin{aligned}
\quad (\boldsymbol{A}^{\text{T}})^{\text{T}} &= \boldsymbol{A} \\
\quad (\boldsymbol{A} + \boldsymbol{B})^{\text{T}} &= \boldsymbol{A}^{\text{T}} + \boldsymbol{B}^{\text{T}} \\
\quad (c\boldsymbol{A})^{\text{T}} &= c\boldsymbol{A}^{\text{T}} \\
\quad (\boldsymbol{A}\boldsymbol{B})^{\text{T}} &= \boldsymbol{B}^{\text{T}} \boldsymbol{A}^{\text{T}}
\end{aligned}
$$

#### 求幂

求幂就是多次乘法，由于乘法的要求，求幂是针对方阵(行数等于列数的矩阵)而言的

对于一般的$$k$$ 次幂，$$\boldsymbol{A}^k$$通过递归定义

$$
\boldsymbol{A}^k = \boldsymbol{A}^{k-1} \cdot \boldsymbol{A}
$$

对于一般的矩阵，求幂的计算量巨大，而对一些特殊形式的矩阵，如对角矩阵、若当标准型、三角矩阵等，它们的求幂运算满足一定规律，可大大减少计算量，因此常将一般矩阵分解变换为这些特殊形式再进行求幂。

#### 求逆

求逆表示求逆变换；满秩的方阵才存在逆，满秩的非方阵存在伪逆。(秩的定义见 2.3)

$$\boldsymbol{A}_{n\times n}$$为可逆矩阵，若存在矩阵$$\boldsymbol{B}$$使得$$\boldsymbol{A} \boldsymbol{B} = \boldsymbol{B} \boldsymbol{A} = \mathbf{I}_{n}$$ ，则称$$\boldsymbol{B}$$为矩阵$$\boldsymbol{A}$$的逆，记为$$\boldsymbol{A}^{-1}$$

一般矩阵求逆的计算量较大，通常将矩阵分解成便于求逆的形式后进行。

高斯若当法是一种通用的手工求逆法：把$$\boldsymbol{A}$$与$$\mathbf{I}$$写一起，通过初等变换将$$\boldsymbol{A}$$变为$$\mathbf{I}$$，原来右边的部分便为$$\boldsymbol{A}^{-1}$$。此外，逆还可以利用伴随矩阵计算

$$
\boldsymbol{A}^{-1} = \frac{1}{\text{det}(\boldsymbol{A})} \boldsymbol{Adj(A)}
$$

伪逆是方阵逆的概念在非方阵中的推广，记为$$A^\dagger$$ ，具体求法此处省略。

#### 分块

某些特殊构型的矩阵分块处理，每个块当作元素进行运算，再分别计算每个块的结果，在手工分析稀疏矩阵时尤其有效。

### 2.2 特殊矩阵

- 单位矩阵：对角元全为 1，非对角元全为 0，记为$$I_{n}$$
- 对角矩阵：非对角元全为 0
- 三角矩阵：主对角元上方或下方的元素全为 0
- 稀疏矩阵：大多数元素为 0
- 对称矩阵：$$\boldsymbol{A}^{\text{T}} = \boldsymbol{A}$$
- 反称矩阵：$$\boldsymbol{A}^{\text{T}} = -\boldsymbol{A}$$，
- 埃尔米特矩阵：$$\boldsymbol{A}^* = \boldsymbol{A}$$ (复数矩阵，共轭转置等于自身)
- 幺正矩阵：$$\boldsymbol{A}^* = \boldsymbol{A}^{-1}$$，即$$\boldsymbol{A}^* \boldsymbol{A} = \mathbf{I}$$
- 正交矩阵：$$\boldsymbol{A}^{\text{T}} = \boldsymbol{A}^{-1}$$，即$$\boldsymbol{A}^{\text{T}}\boldsymbol{A} = \mathbf{I}$$
- 幂等矩阵：$$\boldsymbol{A}^2 = \boldsymbol{A}$$，常用于投影
- 初等矩阵：高斯消元法中初等变换对应的矩阵
- 正定矩阵：对任意非零向量$$\boldsymbol{x}$$，满足$$\boldsymbol{x}^{\text{T}}\boldsymbol{A}\boldsymbol{x} > 0$$的对称矩阵
- 施密特矩阵：用于施密特正交化的矩阵，具有特定的对称性和正交性。
- 幺矩阵：所有行和列元素的和均为 1 ，常用于概率和统计。
- 投影矩阵：$$\boldsymbol{b}$$向$$\boldsymbol{a}$$上的投影为$$\boldsymbol{p} = \frac{\boldsymbol{a}\boldsymbol{a}^{\text{T}}}{\boldsymbol{a}^{\text{T}}\boldsymbol{a}}\boldsymbol{b}$$，是对称矩阵，幂等矩阵；投影矩阵用于解决无解时找列空间中最近的解

### 2.3 矩阵的秩

自然定义域下，$$\boldsymbol{A}\boldsymbol{x} = \boldsymbol{y}$$的值域称为$$\boldsymbol{A}$$的列空间，值域的维度就是矩阵的秩(秩的又一种定义)，记为$$\text{rank}(\boldsymbol{A})$$或$$\text{r}(\boldsymbol{A})$$。列空间$$\text{R}(\boldsymbol{A})$$的秩和行空间$$\text{R}^{\text{T}}(\boldsymbol{A})$$的秩分别称为列秩和行秩。但行秩其实恒等于列秩，故一般统称为矩阵的秩。当$$\boldsymbol{A}_{m\times n}$$满足$$\text{r}(\boldsymbol{A}) = \min\{m, n\}$$时，称$$\boldsymbol{A}$$是满秩的。矩阵可以看作一种线性映射：列满秩对应单射、行满秩对应满射、既列满秩又行满秩对应双射。初等变换不改变矩阵的秩。

矩阵的秩满足一些基本性质

$$
\begin{aligned} &\text{r}(\boldsymbol{A}\boldsymbol{B}) \leqslant \min\{\text{r}(\boldsymbol{A}), \text{r}(\boldsymbol{B})\} \\ &\text{r}(\boldsymbol{A} + \boldsymbol{B}) = \text{r}(\boldsymbol{A}) + \text{r}(\boldsymbol{B}) \\ &\text{r}(\boldsymbol{A}) = \text{r}(\boldsymbol{P}\boldsymbol{A}) = \text{r}(\boldsymbol{A}\boldsymbol{Q}) = \text{r}(\boldsymbol{P}\boldsymbol{A}\boldsymbol{Q}) \\ &\text{r}(\boldsymbol{A}) + \text{r}(\boldsymbol{B}) - n \leqslant \text{r}(\boldsymbol{A}\boldsymbol{B})\quad(西尔维斯特不等式) \end{aligned}
$$

## 第三章 线性方程组

线性代数的另一个研究出发点，是求解线性方程。

$$
\begin{aligned}
\begin{cases}
3x_{1} + 1x_{2} &= -1\\
1x_{1} + 2x_{2} &= 3
\end{cases}\\
\begin{pmatrix}
3 \quad 1 \\
1 \quad 2
\end{pmatrix}
\begin{pmatrix}
x_{1} \\
x_{2}
\end{pmatrix}=
\begin{pmatrix}
-1 \\
3
\end{pmatrix}\\
\end{aligned}
$$

$$
\boldsymbol{A}\boldsymbol{x} = \boldsymbol{y}\\ \boldsymbol{y} = \boldsymbol{b}\\
$$

$$\boldsymbol{A}\boldsymbol{x} = \boldsymbol{y}$$对应的齐次方程为$$\boldsymbol{A}\boldsymbol{x} = \boldsymbol{0}$$，齐次方程的解称为$$\boldsymbol{A}$$的零空间$$\text{null}({\boldsymbol{A}})$$，其维数为$$n - r$$

设矩阵的列数为$$n$$，秩为$$r$$，有秩零定理$$n = r + (n-r)$$

### 3.1 解的性质

关于解的存在性：在自然定义域下，$$\boldsymbol{A}\boldsymbol{x} = \boldsymbol{y}$$的值域称为$$\boldsymbol{A}$$的列空间，若$$\boldsymbol{b}$$在列空间中则方程组有解。另外的判断方式有：若系数矩阵和增广矩阵等秩则方程有解。当$$\boldsymbol{A}\boldsymbol{x} = \boldsymbol{y}$$为单射/系数矩阵行满秩时，解是唯一的。当解唯一时，可直接求出闭式解$$\boldsymbol{x} = \boldsymbol{A}^{-1}\boldsymbol{b}$$

非齐次线性方程解的结构为特解(平移了多少)+ 对应齐次方程通解(零空间的解)。

### 3.2 解法

解线性方程组的通用方法有：基于初等变换的高斯消元法和基于行列式的克莱姆法则。高斯消元法使用初等变换(三个初等行变换+三个初等列变换)将增广矩阵化为标准型，写出方程组的解，一般用于手动计算。克莱姆法则用于解唯一的线性方程组，要求$$\boldsymbol{A}$$为方阵，有特定的公式，计算两个特殊行列式之比即可直接得出解。

![image-20250323212739693](https://raw.githubusercontent.com/SparkyXXX/Hatrix-s-Blog-Image/refs/heads/main/img/image-20250323212739693.png)

## 第四章 行列式

行列式是方阵的一种特征，表征线性变换对空间伸缩的尺度，记为$$\text{det}(\boldsymbol{A})$$或$$ \vert A \vert $$。这种定量的尺度说明，在低维被称作有向面积/有向体积。非方阵没有行列式这一概念。

从行列式的几何意义可以得到一些显然的性质

- $$ \vert \boldsymbol{A}^{\text{T}} \vert = \vert \boldsymbol{A} \vert $$，即转置不影响行列式
- 某一行/列乘$$k$$，行列式变为$$k$$倍；$$ \vert k\boldsymbol{A} \vert = k^n \vert \boldsymbol{A} \vert $$
- 行列互换，正负号改变
- 行列倍加，行列式不变
- $$ \vert \boldsymbol{A} \vert \ne 0$$，$$\boldsymbol{A}$$满秩，$$\boldsymbol{A}$$可逆三个概念相互等价

其它性质

- $$ \vert \boldsymbol{A}+\boldsymbol{B} \vert = \vert \boldsymbol{A} \vert + \vert \boldsymbol{B} \vert $$，和的行列式=行列式的和
- $$ \vert \boldsymbol{A}\boldsymbol{B} \vert = \vert \boldsymbol{A} \vert \vert \boldsymbol{B} \vert $$，积的行列式=行列式的积

### 4.1 定义

二阶行列式的定义相对简单，且可以直接计算

$$
{\text{det}}(\boldsymbol{A}) = \begin{vmatrix} a_{11} & a_{12} \\ a_{21} & a_{22} \end{vmatrix} = a_{11}a_{22} - a_{12}a_{21}
$$

$$n$$阶行列式的一般定义涉及逆序数和全排列，较为复杂

- 逆序数：在一个排列中，对于两个元素$$a_{i}$$和$$a_{j}$$，如果$$i < j$$且$$a_{i} > a_{j}$$，则称这对元素为一个逆序对。逆序数就是逆序对的数量，可描述一个排列的“混乱程度”。正式定义为

  给定一个长度为$$n$$的排列$$a_{1}, a_{2}, \ldots, a_{n}$$其逆序数$$I$$定义如下，其中$$\mathbb{I}(P)$$是指示函数，当 $$P$$为真时取值 1，否则取值 0

$$
I = \sum_{1 \leqslant i < j \leqslant n} \mathbb{I}(a_{i} > a_{j})
$$

给定$$\boldsymbol{A}_{n\times n}$$，其行列式为

$$
\boldsymbol{A} = \begin{pmatrix} a_{11} & a_{12} & \cdots & a_{1n} \\ a_{21} & a_{22} & \cdots & a_{2n} \\ \vdots & \vdots & \ddots & \vdots \\ a_{n1} & a_{n2} & \cdots & a_{nn} \end{pmatrix} \quad {\text{det}}(\boldsymbol{A}) = \sum_{\sigma \in S_{n}} \text{sgn}(\sigma) \prod_{i=1}^{n} a_{i, \sigma(i)}
$$

其中$$S_{n}$$为$$n$$的全排列集合，其中的元素为一组有序数组的各个排列方式。$$\text{sgn}(\sigma)$$为排列的符号，

- 如果排列是偶排列(通过偶数次交换相邻元素得到自然顺序序列的排列)，则$$\text{sgn}(\sigma) = +1$$
- 如果排列是奇排列(通过奇数次交换相邻元素得到自然顺序序列的排列)，则$$\text{sgn}(\sigma) = -1$$

例如，对于三阶方阵

$$
{\text{det}}(\boldsymbol{A}) = a_{11} (a_{22} a_{33} - a_{23} a_{32}) - a_{12} (a_{21} a_{33} - a_{23} a_{31}) + a_{13} (a_{21} a_{32} - a_{22} a_{31})
$$

高阶行列式一般用拉普拉斯展开进行计算(用递推的方式降阶计算)

$$
{\text{det}}(\boldsymbol{A}) = \sum_{i=1}^{n} (-1)^{i+j} a_{ij} \cdot{\text{det}}(\boldsymbol{A}_{ij})
$$

### 4.2 相关概念

与行列式相关的定义还有

- 子式：取出$$\boldsymbol{A}$$的某些行和列，组成$$k$$阶行列式
- 主子式：取的行号列号相同
- 顺序主子式：取的行号列号相同且不跳号(矩阵左上角的方阵的行列式)
- 余子式：去除某行某列剩余部分的行列式
- 代数余子式：在余子式的基础上乘$$(-1)^{i+j}$$
- 伴随矩阵：代数余子式按照原排列构成矩阵

这里又可以给出秩的另一个概念：秩就是最高阶非零子式的阶数

## 第五章 矩阵分解

### 5.1 对角化

正式介绍矩阵分解前，需要先介绍线性代数中的一种重要技术：对角化。对角矩阵由于其特殊的结构，拥有求幂求逆等运算简单，各列/各行的向量在向量空间中直接就是正交基等十分优越的性质，使得在许多场景下，都会先将原矩阵变换为对角矩阵再进行后续分析。 相似对角化和合同对角化是两种常见的对角化方式。

#### 相似对角化

向量空间的基代表一种视角，为了简化问题，需要基变换和对应的坐标变换。同一个变换在不同视角下的表示便互为相似矩阵。设$$\{\boldsymbol{m_{1}}、\boldsymbol{m_{2}}、\boldsymbol{m_{3}}\cdots\boldsymbol{m_{s}}\}$$和$$\{\boldsymbol{n_{1}}、\boldsymbol{n_{2}}、\boldsymbol{n_{3}}\cdots\boldsymbol{n_{s}}\}$$是同一个向量空间的两个基，则存在唯一的矩阵$$\boldsymbol{P}$$，使得

$$
\begin{aligned}
(\boldsymbol{n_{1}}、\boldsymbol{n_{2}}、\boldsymbol{n_{3}}\cdots\boldsymbol{n_{s}})&=(\boldsymbol{m_{1}}、\boldsymbol{m_{2}}、\boldsymbol{m_{3}}\cdots\boldsymbol{m_{s}})\cdot \boldsymbol{P}\\
[x]_{n} &= \boldsymbol{P}^{-1}[x]_{M}
\end{aligned}
$$

这称为基变换和坐标变换，$$\boldsymbol{P}$$称为过渡矩阵。

$$\boldsymbol{A}$$和$$\boldsymbol{B}$$均为方阵，相似的定义为，存在可逆矩阵$$\boldsymbol{P}$$使得$$\boldsymbol{B} = \boldsymbol{P}^{-1} \boldsymbol{A} \boldsymbol{P}$$ ，记为$$\boldsymbol{A}\sim\boldsymbol{B}$$，$$\boldsymbol{P}$$称为相似变换矩阵。若$$\boldsymbol{A}$$是一对角矩阵，那么从$$\boldsymbol{B}$$转化到$$\boldsymbol{A}$$的过程就是相似对角化。若$$\boldsymbol{P}$$为正交矩阵，则称为正交对角化，相应的说明$$\boldsymbol{A}$$为实对称矩阵。相似只是在不同视角下的不同表达，实质是相同的。相似变换不改变矩阵的迹。

若$$\boldsymbol{A}\sim\boldsymbol{B}$$，则它们的$$k$$次幂、转置、共轭转置都是相似的。此外，相似还具有传递性。相似矩阵的特征值、行列式、迹均相等。

#### 合同对角化

二次型是一种实际工程中常用的关于向量变量多项式

$$
Q(\boldsymbol{x})=\boldsymbol{x}^{\text{T}}\boldsymbol{A}\boldsymbol{x}
$$

其中$$\boldsymbol{x}_{n \times 1}$$为列向量，$$\boldsymbol{A}_{n \times n}$$为对称矩阵，称为$$\boldsymbol{A}$$为$$Q(\boldsymbol{x})$$的二次型矩阵。

例如

$$
Q(x, y) = ax^2 + 2bxy + cy^2\\ Q(\boldsymbol{x}) = \begin{pmatrix} x \\ y \end{pmatrix}^{\text{T}} \begin{pmatrix} a & b \\ b & c \end{pmatrix} \begin{pmatrix} x \\ y \end{pmatrix}
$$

二次型可用于处理二次齐次多项式，如旋转一个椭圆。只含平方项的二次型称为标准形。若$$\boldsymbol{x}$$任意取值均满足$$Q(\boldsymbol{x}) > 0$$，则称二次型$$Q(\boldsymbol{x})$$是正定的，类似的定义还有负定、半正定、半负定、不定。判定正定的一种方式为赫尔维茨定理：$$Q(\boldsymbol{x})$$正定的充要条件为二次型矩阵的各阶顺序主子式为正。二次型正定/负定代表有全局最小/最大值。

$$\boldsymbol{A}$$和$$\boldsymbol{B}$$均为方阵，合同的定义为，存在可逆矩阵$$\boldsymbol{P}$$使得$$\boldsymbol{B} = \boldsymbol{P}^{\text{T}}\boldsymbol{A} \boldsymbol{P}$$ ，记为$$\boldsymbol{A} \cong \boldsymbol{B}$$ ，$$\boldsymbol{P}$$称为合同变换矩阵。若$$\boldsymbol{A}$$是一对角矩阵，那么从$$\boldsymbol{B}$$转化到$$\boldsymbol{A}$$的过程就是合同对角化。

合同变换对二次型而言相当于进行坐标变换，合同对角化相当于通过坐标变换去掉了交叉项。合同对角化的方法有正交对角化和配方法。

设$$\boldsymbol{A} = \boldsymbol{P}\boldsymbol{\Lambda}\boldsymbol{P}^{\text{T}}$$，$$\boldsymbol{\Lambda}$$为对角矩阵，则

$$
Q(\boldsymbol{x})=\boldsymbol{x}^{\text{T}}\boldsymbol{A}\boldsymbol{x} = \boldsymbol{x}^{\text{T}}\boldsymbol{P}\boldsymbol{\Lambda}\boldsymbol{P}^{\text{T}}\boldsymbol{x} = (\boldsymbol{P}^{\text{T}}\boldsymbol{x})^{\text{T}}\boldsymbol{\Lambda}(\boldsymbol{P}^{\text{T}}\boldsymbol{x}) = \boldsymbol{y}^{\text{T}}\boldsymbol{\Lambda}\boldsymbol{y}
$$

### 5.2 矩阵分解

![image-20250323213009587](https://raw.githubusercontent.com/SparkyXXX/Hatrix-s-Blog-Image/refs/heads/main/img/image-20250323213009587.png)

常见的矩阵分解有图中的五种方式，这些分解方式提供了多种看待矩阵的方式，其中应用最广泛、功能最强大的是：特征值分解和奇异值分解。奇异值又可以看作特征值分解的在非方阵的推广。

#### 特征值分解

ED(Eigenvalue Decomposition)

给定一个的实对称矩阵$$\boldsymbol{A}_{n \times n}$$，特征值分解的形式为：

$$
\boldsymbol{A} = \boldsymbol{P} \boldsymbol{\Lambda} \boldsymbol{P}^{-1}
$$

在线性变换中只缩放而方向不变的向量为特征向量(旋转矩阵没有实数特征值)、对应的缩放系数为特征值，即

$$
\boldsymbol{A}\boldsymbol{x} = \lambda\boldsymbol{x}
$$

特征多项式中根的重数称为该特征值的代数重数，特征向量的数量(对应特征空间的维度)称为该特征值的几何重数。不同特征值对应的特征向量线性无关。

计算方法：求解特征多项式 $$ \vert \boldsymbol{A}−λ\mathbf{I} \vert =0$$ 获得特征值，代入原方程得到特征向量。特征值构成对角矩阵$$\boldsymbol{\Lambda}$$，特征向量构成相似变换矩阵$$\boldsymbol{P}$$，如此便完成了相似对角化$$\boldsymbol{A} = \boldsymbol{P}\boldsymbol{\Lambda}\boldsymbol{P}^{-1}$$

例如

$$
\boldsymbol{A} = \begin{pmatrix} 4 & 1 \\ 2 & 3 \end{pmatrix}
$$

首先计算特征值

$$
\begin{aligned}
&\boldsymbol{A} - \lambda \mathbf{I} = \begin{pmatrix} 4 - \lambda & 1 \\ 2 & 3 - \lambda \end{pmatrix} \\
&{\text{det}}(\boldsymbol{A} - \lambda \mathbf{I}) = (4 - \lambda)(3 - \lambda) - 2 \cdot 1 = 0 \\
&\lambda_{1} = 5, \quad \lambda_{2} = 2
\end{aligned}
$$

后计算特征向量

$$
\begin{aligned}
&(\boldsymbol{A} - 5\mathbf{I})\boldsymbol{v} = 0 \\
&\begin{pmatrix} 4 - 5 & 1 \\ 2 & 3 - 5 \end{pmatrix} \begin{pmatrix} x \\ y \end{pmatrix} = \begin{pmatrix} -1 & 1 \\ 2 & -2 \end{pmatrix} \begin{pmatrix} x \\ y \end{pmatrix} = \begin{pmatrix} 0 \\ 0 \end{pmatrix} \\
&解得−x+y=0\\
&\boldsymbol{v}_{1} = k \begin{pmatrix} 1 \\ 1 \end{pmatrix} \quad (k \neq 0)\\
&同理得\lambda_{2} = 2对应的特征向量: \boldsymbol{v}_{2} = k \begin{pmatrix} 1 \\ -2 \end{pmatrix} \quad (k \neq 0)
\end{aligned}
$$

构造变换矩阵和对角矩阵

$$
\begin{aligned}
&\boldsymbol{P} = \begin{pmatrix} 1 & 1 \\ 1 & -2 \end{pmatrix}, \quad \boldsymbol{\Lambda} = \begin{pmatrix} 5 & 0 \\ 0 & 2 \end{pmatrix} \\
&\boldsymbol{A} = \boldsymbol{P} \boldsymbol{\Lambda} \boldsymbol{P}^{-1}
\end{aligned}
$$

由于正交矩阵具有逆等于转置的良好性质，又总结出了施密特正交化的一般方法，可借助向量空间下的一个基找到该空间下的正交基。对特征向量采用施密特正交化,可将相似变换矩阵$$\boldsymbol{P}$$变为正交矩阵$$\boldsymbol{Q}$$，具体公式很容易查到，此处暂不给出。

#### 奇异值分解

SVD(Singlevalue Decomposition)

![image-20250323213133455](https://raw.githubusercontent.com/SparkyXXX/Hatrix-s-Blog-Image/refs/heads/main/img/image-20250323213133455.png)

给定$$\boldsymbol{A}_{m \times n}$$，可表示为三个特定矩阵的乘积，其中$$\boldsymbol{U}_{m \times m}$$是正交矩阵，其列向量称为左奇异向量；$$\boldsymbol{V}_{n \times n}$$是正交矩阵，其列向量称为右奇异向量；$$\boldsymbol{\Sigma}_{m \times n}$$是对角矩阵，其中的非负实数称为奇异值。

$$
\boldsymbol{A} = \boldsymbol{U} \boldsymbol{\Sigma} \boldsymbol{V}^{\text{T}}
$$

- 奇异向量均为单位向量。
- 奇异值是$$\boldsymbol{A}$$特征值的平方根
- 奇异值的数量等于$$\text{r}(\boldsymbol{A})$$

计算方法：求$$\boldsymbol{A}^{\text{T}} \boldsymbol{A}$$，求解特征值问题$$\boldsymbol{A}^{\text{T}} \boldsymbol{A} \boldsymbol{v} = \lambda \boldsymbol{v}$$，奇异值$$\sigma_{i} = \sqrt{\lambda_{i}}$$；将特征值$$\lambda_\text{i}$$代入原方程得到右奇异向量$$\boldsymbol{v}_\text{i}$$，并规范化为单位向量。再利用$$\boldsymbol{u}_{i} = \frac{1}{\sigma_{i}} \boldsymbol{A} \boldsymbol{v}_{i}$$计算得到左奇异向量。将奇异向量和奇异值排列成对应矩阵便得到分解结果。

例如

$$
\boldsymbol{A} = \begin{pmatrix} 3 & 1 & 2 \\ 2 & 4 & 1 \end{pmatrix}
$$

首先计算奇异值

$$
\begin{aligned}
&\boldsymbol{A}^{\text{T}} \boldsymbol{A} = \begin{pmatrix} 3 & 2 \\ 1 & 4 \\ 2 & 1 \end{pmatrix} \begin{pmatrix} 3 & 1 & 2 \\ 2 & 4 & 1 \end{pmatrix} = \begin{pmatrix} 13 & 11 & 9 \\ 11 & 17 & 6 \\ 9 & 6 & 5 \end{pmatrix}\\
&\boldsymbol{A}^{\text{T}} \boldsymbol{A} - \lambda \mathbf{I} = \begin{pmatrix} 13 - \lambda & 11 & 9 \\ 11 & 17 - \lambda & 6 \\ 9 & 6 & 5 - \lambda \end{pmatrix}\\
&{\text{det}}(\boldsymbol{A}^{\text{T}} \boldsymbol{A} - \lambda \mathbf{I}) = 0\\
&-\lambda^3 + 35\lambda^2 - 362\lambda + 1004 = 0\\
&解得\lambda_{1} \approx 29.29, \quad \lambda_{2} \approx 4.71, \quad \lambda_{3} \approx 0.01\\
&\sigma_{1} = \sqrt{29.29} \approx 5.41, \quad \sigma_{2} = \sqrt{4.71} \approx 2.17, \quad \sigma_{3} = \sqrt{0.01} \approx 0.1
\end{aligned}
$$

后计算奇异向量

$$
\begin{aligned}
&(\boldsymbol{A}^{\text{T}} \boldsymbol{A} - 29.29 \mathbf{I}) \boldsymbol{v} = 0 \\
&解得\boldsymbol{v}_{1} \approx\begin{pmatrix} 0.78 \\ 0.62 \\ 0.13 \end{pmatrix}\\
&同理解得\boldsymbol{v}_{2} \approx \begin{pmatrix} -0.62 \\ 0.78 \\ 0.13 \end{pmatrix} \quad \boldsymbol{v}_{3} \approx \begin{pmatrix} 0.13 \\ 0.13 \\ -0.97 \end{pmatrix} \\
&利用公式\boldsymbol{u}_{i} = \frac{1}{\sigma_{i}} \boldsymbol{A} \boldsymbol{v}_{i}求得：\\
&\boldsymbol{u}_{1} = \frac{1}{5.41} \boldsymbol{A} \boldsymbol{v}_{1} \approx \frac{1}{5.41} \begin{pmatrix} 3 & 1 & 2 \\ 2 & 4 & 1 \end{pmatrix} \begin{pmatrix} 0.78 \\ 0.62 \\ 0.13 \end{pmatrix} \approx \begin{pmatrix} 0.73 \\ 0.51 \end{pmatrix}\\
&\boldsymbol{u}_{2} = \frac{1}{2.17} \boldsymbol{A} \boldsymbol{v}_{2} \approx \frac{1}{2.17} \begin{pmatrix} 3 & 1 & 2 \\ 2 & 4 & 1 \end{pmatrix} \begin{pmatrix} -0.62 \\ 0.78 \\ 0.13 \end{pmatrix} \approx \begin{pmatrix} -0.54 \\ 0.75 \end{pmatrix}\\
&\boldsymbol{u}_{3} = \frac{1}{0.1} \boldsymbol{A} \boldsymbol{v}_{3} \approx \frac{1}{0.1} \begin{pmatrix} 3 & 1 & 2 \\ 2 & 4 & 1 \end{pmatrix} \begin{pmatrix} 0.13 \\ 0.13 \\ -0.97 \end{pmatrix} \approx \begin{pmatrix} 2.5 \\ -0.8 \end{pmatrix}
\end{aligned}
$$

构造三个矩阵

$$
\begin{aligned}
\boldsymbol{V} &= \begin{pmatrix} 0.78 & -0.62 & 0.13 \\ 0.62 & 0.78 & 0.13 \\ 0.13 & 0.13 & -0.97 \end{pmatrix} \\
\boldsymbol{\Sigma} &= \begin{pmatrix} 5.41 & 0 & 0 \\ 0 & 2.17 & 0 \\ 0 & 0 & 0.1 \end{pmatrix}\\
\boldsymbol{U} &= \begin{pmatrix} 0.73 & -0.54 & 2.5 \\ 0.51 & 0.75 & -0.8 \end{pmatrix}\\
\boldsymbol{A} &= \boldsymbol{U} \boldsymbol{\Sigma} \boldsymbol{V}^{\text{T}}
\end{aligned}
$$

特征值分解和奇异值分解的常见应用有：主成分分析、数据压缩、特征提取等。

## 伴随矩阵的意义

伴随矩阵(Adjugate Matrix)，在数学和物理中有许多重要的含义，尤其是在与矩阵的逆、线性变换、几何以及物理学中的应用中具有深刻的意义。以下是伴随矩阵的一些物理及深层次含义：

### 1. 矩阵逆的计算

伴随矩阵的主要用途之一是帮助计算矩阵的逆。在代数中，给定一个可逆矩阵 $$\boldsymbol{A}$$，其逆矩阵可以通过伴随矩阵来表示：

$$
\boldsymbol{A}^{-1} = \frac{1}{\det(\boldsymbol{A})} \boldsymbol{Adj(A)}
$$

这里，$$\boldsymbol{Adj(A)}$$是矩阵 $$\boldsymbol{A}$$的伴随矩阵，$$\det(\boldsymbol{A})$$ 是矩阵的行列式。该公式的深层次含义在于，伴随矩阵和行列式共同构成了矩阵可逆的条件。行列式的几何解释是体积缩放因子，而伴随矩阵则与矩阵的协变变换相关。

### 2. 几何解释

伴随矩阵与几何学中的线性变换密切相关。在线性代数中，矩阵代表线性变换，而伴随矩阵与这些变换的协变性质有关。

#### (1) 线性变换的几何解释

伴随矩阵的元素是由每个元素的代数余子式构成的。几何上，可以将伴随矩阵理解为一个映射，它作用于 n-1 维空间(例如，对于三维空间，它作用在二维平面上)，调整方向、面积或体积。

具体来说：

- 在 $$\mathbb{R}^2$$ 中，伴随矩阵作用于原始线性变换下的线段，保持线性变换下方向的关系。
- 在 $$\mathbb{R}^3$$ 中，伴随矩阵可以作用在变换后的平面上，调整该平面的法向量，并且能够调整体积的缩放。

伴随矩阵与原矩阵表示了在几何变换中某种对偶的关系。伴随矩阵的列(或行)通常与原矩阵中平面的法向量相关。

#### (2) 伴随矩阵与体积的关系

伴随矩阵的几何意义还可以通过其与体积的关系来解释。考虑一个 $$n \times n$$ 的矩阵 $$\boldsymbol{A}$$，它将一个单位立方体映射到某个体积为 $$\det(\boldsymbol{A})$$ 的平行六面体。伴随矩阵的列向量与原矩阵的每一个列向量张成的超平面的法向量相关。因此，伴随矩阵不仅与线性变换的逆密切相关，还与变换后的体积和方向密切关联。

### 3. 物理中的应用

伴随矩阵在物理学中的多种领域都有应用。以下是一些主要的物理背景：

#### (1) 经典力学中的转动矩阵

在经典力学中，伴随矩阵可以在描述刚体旋转、转动惯量矩阵等问题中找到应用。在三维空间中，伴随矩阵作用在矩阵的子空间上，使得我们能够用它来计算刚体旋转的逆问题。例如，在三维刚体的动力学分析中，伴随矩阵可以帮助分析惯性张量。

#### (2) 电磁场和线性代数

伴随矩阵的某些形式也出现在描述电磁场和电场势的问题中。电磁场中涉及的对称张量或反对称张量有时可以通过伴随矩阵进行变换和处理。

#### (3) 量子力学中的矩阵表示

在量子力学中，矩阵形式广泛用于表示可观测量和态的演化。伴随矩阵可以用于计算某些特定操作的逆，尤其是在某些对称性和群论的应用中，伴随矩阵的概念对某些算符代数起到重要的作用。

### 4. 矩阵的代数性质

在代数中，伴随矩阵也是一些重要的矩阵恒等式中的关键成分。例如，伴随矩阵具有如下性质：

$$
\boldsymbol{A} \cdot \boldsymbol{Adj(A)} = \boldsymbol{Adj(A)} \cdot \boldsymbol{A} = \det(\boldsymbol{A}) \mathbf{I}
$$

这意味着原矩阵与其伴随矩阵的乘积会产生一个以行列式为缩放因子的单位矩阵。这反映了伴随矩阵与矩阵逆之间的代数关系。

### 5. 线性方程组的解

伴随矩阵还用于解决线性方程组。假设有线性方程组 $$\boldsymbol{A}\boldsymbol{x} = \boldsymbol{b}$$，其中$$\boldsymbol{A}$$是一个可逆矩阵，通过伴随矩阵和行列式，可以表示解为：

$$
\boldsymbol{x} = \frac{1}{\det(\boldsymbol{A})} \boldsymbol{Adj(A)} \boldsymbol{b}
$$

这意味着我们可以通过伴随矩阵来求解方程组的解。尽管实际计算中，伴随矩阵方法不如其他数值方法高效，但它仍然提供了代数上的解析解法。

### 总结

伴随矩阵不仅在计算矩阵的逆时非常有用，它还具有重要的几何和物理意义：

- 几何上，它与线性变换、体积和方向的调整密切相关。
- 物理上，它在刚体旋转、电磁场和量子力学等领域有具体应用。
- 从代数的角度看，伴随矩阵和行列式一起构成了矩阵可逆性的重要工具，并且出现在许多代数恒等式中。

这些深层次的含义表明，伴随矩阵不仅是一个计算工具，还在数学和物理中的许多领域具有重要的解释性作用。

## 线性代数在数据统计中的应用

线性代数在数据统计中的应用非常广泛，特别是在多元数据分析、回归分析、主成分分析(PCA)和机器学习等领域。以下是一些常见的应用场景和例子，展示了线性代数如何在数据统计中发挥重要作用。

### 1. 回归分析中的最小二乘法

在回归分析中，我们常常希望通过拟合一条直线(或高维的超平面)来最小化预测值与观测值之间的差异。这个问题可以通过最小二乘法来解决，而最小二乘法的解正是基于线性代数中的矩阵运算。

例：有一个线性回归模型，模型形式为：

$$
y = X\beta + \epsilon
$$

- $$y$$ 是 $$n \times 1$$ 的观测数据向量，
- $$X$$ 是 $$n \times p$$ 的设计矩阵，其中 $$n$$ 是样本数， $$p$$ 是自变量的个数，
- $$\beta$$ 是 $$p \times 1$$ 的回归系数向量，
- $$\epsilon$$ 是误差项向量。

我们要估计回归系数 $$\beta$$，最小二乘法的目标是最小化残差平方和：

$$
\min_{\beta} \| y - X\beta \|^2
$$

使用线性代数的方法，可以通过解下面的矩阵方程来得到回归系数的估计：

$$
\hat{\beta} = (X^\top X)^{-1} X^\top y
$$

这就是利用线性代数中的矩阵求逆与转置来获得最小二乘解的过程。

### 2. 主成分分析(PCA)

主成分分析(PCA)是高维数据降维的常用方法，能够在保持数据方差的情况下降低维度。PCA 的核心是利用线性代数中的特征值分解或奇异值分解(SVD)来找到数据的主成分。

例：有一个矩阵 $$X$$ ，表示 $$n$$ 个样本和 $$p$$ 个变量。我们可以通过以下步骤使用 PCA 来降低数据维度：

1. 计算数据矩阵的协方差矩阵 $$\Sigma = \frac{1}{n-1} X^\top X$$ 。
2. 对协方差矩阵 $$\Sigma$$ 进行特征值分解，得到特征值和特征向量。
3. 选择具有最大特征值的前 $$k$$ 个特征向量(主成分)，这些特征向量对应的数据方向解释了最多的方差。
4. 用这些特征向量对原始数据进行线性变换，从而实现数据的降维。

通过这种方式，PCA 利用线性代数的矩阵对角化或奇异值分解技术，将高维数据投影到低维空间。

### 3. 协方差矩阵与相关性矩阵

在统计学中，协方差矩阵和相关性矩阵是两个非常重要的概念，用于衡量多个变量之间的线性关系。它们是通过矩阵运算来定义的，且经常在多元分析中使用。

例：有一个 $$n \times p$$ 的数据矩阵 $$X$$ ，表示 $$n$$ 个样本的 $$p$$ 个变量。协方差矩阵 $$\Sigma$$ 的定义是：

协方差矩阵是一个 p \times p 的矩阵，每个元素 \sigma\_{ij} 表示变量 X_i 和 X_j 之间的协方差。协方差矩阵是对称矩阵，其主对角线上的元素表示每个变量的方差。

$$
\Sigma = \frac{1}{n-1} X^\top X
$$

通过协方差矩阵的特征值分解，我们可以获得该矩阵的主成分，即找到数据中解释方差最多的方向。

### 4. 多元数据分析中的距离度量

在聚类分析和分类问题中，数据点之间的距离是非常重要的一个度量。常用的欧氏距离可以用线性代数的内积来表示。

例：有两个样本 $$x = (x_1, x_2, \dots, x_p)$$ 和 $$y = (y_1, y_2, \dots, y_p)$$ ，它们之间的欧氏距离可以用线性代数中的范数来表示：

$$
d(x, y) = \|x - y\|\_2 = \sqrt{(x_1 - y_1)^2 + (x_2 - y_2)^2 + \dots + (x_p - y_p)^2}
$$

这其实是向量 $$x - y$$ 的 $$L_2$$ 范数，也就是向量的长度。

在高维数据中，欧氏距离可以通过矩阵运算来快速计算，这在数据的聚类、分类等问题中非常常见。

### 5. 奇异值分解(SVD)在数据压缩中的应用

奇异值分解(SVD)是线性代数中非常强大的工具，它可以将矩阵分解为三个部分，用于许多领域，如数据压缩、推荐系统和特征提取。

例：有一个矩阵 $$X$$ ，可以通过 SVD 分解为：

$$
X = U \Sigma V^\top
$$

这里：

- $$U$$ 是左奇异向量矩阵，
- $$\Sigma$$ 是对角矩阵，其对角元素是奇异值，
- $$V^\top$$ 是右奇异向量矩阵。

通过截断较小的奇异值，我们可以压缩数据。例如，在图像处理中，SVD 用于图像压缩，保留最大的奇异值并忽略较小的奇异值，从而减少存储空间但保持图像的主要特征。

### 6. 线性代数在机器学习中的应用

在机器学习中，线性代数是基础工具，很多机器学习算法都依赖于矩阵运算，如梯度下降法中的矩阵求导、支持向量机中的核矩阵计算等。

例：在训练机器学习模型时，我们往往需要通过梯度下降法来最小化损失函数。假设损失函数是线性回归中的均方误差：

$$
L(\beta) = \frac{1}{n} \sum\_{i=1}^{n} (y_i - X_i \beta)^2
$$

利用线性代数，可以对参数 $$\beta$$ 求导并更新模型：

$$
\nabla L(\beta) = -\frac{2}{n} X^\top (y - X \beta)
$$

这里的梯度计算和参数更新是典型的矩阵运算。

不改变内积的矩阵变换称为正交变换，对应正交矩阵

矩阵指数

欧拉公式的矩阵写法，其中$$\begin{bmatrix}
0 & -1\\
1 & 0
\end{bmatrix}$$为 90° 旋转矩阵，与虚数单位$$i$$作用相同

$$
\text{exp}\left(
\begin{bmatrix}
0 & -1\\
1 & 0
\end{bmatrix}\pi
\right) =
\begin{bmatrix}
-1 & 0\\
0 & -1
\end{bmatrix}=-\boldsymbol{I}
$$

90° 旋转矩阵乘上时间$$t$$，在一起作指数运算，就得到了这个空间下的任意旋转矩阵(泰勒展开可以证明)

$$
\text{exp}\left(
\begin{bmatrix}
0 & -1\\
1 & 0
\end{bmatrix}t
\right) =
\begin{bmatrix}
\cos t & -\sin t\\
\sin t & \cos t
\end{bmatrix}=-\boldsymbol{I}
$$

另外，矩阵指数是一类特殊形式微分方程的解

对于方程

$$
\frac{\text{d}}{\text{d}t}
\begin{bmatrix}
x\\
y
\end{bmatrix}
=
\begin{bmatrix}
a & b\\
c & d
\end{bmatrix}
\begin{bmatrix}
x\\
y
\end{bmatrix}
$$

解为

$$
\begin{bmatrix}
x\\
y
\end{bmatrix}
=
\text{exp}\left(
\begin{bmatrix}
a & b\\
c & d
\end{bmatrix}t
\right)
\cdot
\begin{bmatrix}
x_0\\
y_0
\end{bmatrix}
$$

对于任意矢量，其坐标的含义就是在坐标轴上的投影是单位长度的多少倍

两个标准正交基之间的变换，既不能改变单位矢量的长度，也不能改变基底之间的夹角，这样的变换代表只有旋转和反演的变换，称为正交变换，对应的矩阵称为正交矩阵。将矢量推广到复数的情况，就对应于幺正变换，以及幺正矩阵(酉矩阵)。

复数矢量$$\boldsymbol{a}$$(向量中每个元素都是复数)的模长为复数向量的共轭转置$$\boldsymbol{a}^*$$乘以这个复数矢量本身，复数列矢量称为右矢($$\boldsymbol{a}=\ket{a}$$)，取了复共轭的行矢量称为左矢($$\boldsymbol{a}^*=\bra{a}$$)，模长(内积)可以表示为$$\braket{a}$$，外积可以表示为$$\ket{a}\bra{a}$$。这种表示方式就是狄拉克符号。

矩阵$$\boldsymbol{A}$$的共轭转置称为厄米共轭$$\boldsymbol{A}^{\dagger}$$，幺正矩阵的厄米共轭与自身相乘为单位矩阵，即$$\boldsymbol{A}^{\dagger}\boldsymbol{A} = \boldsymbol{I}$$

$$\delta_{ij}$$表示$$i=j$$时结果为 1，$$i\neq j$$时结果为 0

线性代数的一个强大之处在于，只要是满足一定条件的数学对象，都可以看成所谓的“矢量”来进行一定操作，甚至可以定义这些矢量之间的投影、内积和外积等，比如两个函数之间的内积。

把一个函数的基底变成一组不同频率的平面波，那么求得不同频率的平面波的成分就可以看作求这个函数矢量在平面波坐标系上的坐标(傅里叶变换)。

同阶的非奇异方阵及其乘法可以构成群，称为 n 阶矩阵乘法群，描述了在 n 维空间中，改变基底只改变矢量的坐标而不改变其数学性质。

在实数域中，行列式为 1 的正交矩阵是旋转矩阵；在复数域中，行列式为 1 的幺正矩阵是旋转矩阵。

对向量进行指数映射，会得到对应流形上的元素

## 推荐资料

[MIT 线性代数](https://www.bilibili.com/video/BV16Z4y1U7oU/?spm_id_from=333.337)

[MIT 线性代数笔记](https://zhuanlan.zhihu.com/p/45707832)

[3Blue1Brown 线性代数的本质](https://www.bilibili.com/video/BV1ys411472E/?spm_id_from=333.999.0.0&vd_source=1d0891b41fe4e23dbf197eaf61dfa468)

[漫氏沉思录 无痛线代](https://www.bilibili.com/video/BV1wu411T7dj/?spm_id_from=333.33)

[轩兔 矩阵论](https://www.bilibili.com/video/BV1PA411T7b5/?spm_id_from=333.788&vd_source=1d0891b41fe4e23dbf197eaf61dfa468)

[Linear Algebra Done Right](https://linear.axler.net/)

[The Art of Linear Algebra](https://gitee.com/kin_professor/The-Art-of-Linear-Algebra)

[克莱姆法则几何解释](https://www.bilibili.com/video/BV1Pb411W7dc/)
